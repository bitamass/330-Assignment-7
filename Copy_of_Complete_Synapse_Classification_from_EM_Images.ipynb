{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnZuTRsxdYj3TB7bUQezY3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bitamass/330-Assignment-7/blob/main/Copy_of_Complete_Synapse_Classification_from_EM_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Synapse Classification from Electron Microscopy Images\n",
        "# Based on the Microns Explorer dataset\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.backend as K\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import h5py\n",
        "import glob\n",
        "import pickle\n",
        "from scipy import ndimage as ndi\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.cm as cm\n",
        "import gc\n",
        "import warnings\n",
        "from functools import partial\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "\n",
        "# Try to import microns API, but continue if not available\n",
        "try:\n",
        "    from microns_utils.access_microns import MicronsCatalog\n",
        "except ImportError:\n",
        "    print(\"Warning: microns_utils not available. Will use placeholder data instead.\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)"
      ],
      "metadata": {
        "id": "kjWjCzjeRz6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc614cf-2a17-4439-f777-202d183f0a3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: microns_utils not available. Will use placeholder data instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SynapseClassifier:\n",
        "    def __init__(self, data_path, output_path, image_size=(224, 224), batch_size=32):\n",
        "        \"\"\"\n",
        "        Initialize the Synapse Classifier\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the Microns Explorer dataset\n",
        "            output_path (str): Path to save models and results\n",
        "            image_size (tuple): Size to resize the input images to\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.output_path = output_path\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "        self.model = None\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Initialize metrics tracking\n",
        "        self.metrics_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}"
      ],
      "metadata": {
        "id": "kfxKkD0FR1Y3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_microns_data(self, sample_limit=None):\n",
        "    \"\"\"\n",
        "    Load and preprocess data from the Microns Explorer dataset\n",
        "\n",
        "    Args:\n",
        "        sample_limit (int, optional): Limit the number of samples to load for testing\n",
        "\n",
        "    Returns:\n",
        "        tuple: Processed images and their corresponding labels\n",
        "    \"\"\"\n",
        "    print(\"Loading Microns Explorer dataset...\")\n",
        "\n",
        "    # Initialize the microns catalog\n",
        "    try:\n",
        "        # Proper Microns dataset access\n",
        "        catalog = MicronsCatalog(self.data_path)\n",
        "        synapse_collection = catalog.get_collection('synapses')\n",
        "\n",
        "        # Get synapses with their labels\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        # Load synapse data in batches to reduce memory usage\n",
        "        batch_size = 100  # Process 100 synapses at a time\n",
        "        for i in tqdm(range(0, synapse_collection.count(), batch_size), desc=\"Loading synapse batches\"):\n",
        "            # Get a batch of synapses\n",
        "            synapse_batch = synapse_collection.get_items(i, min(i + batch_size, synapse_collection.count()))\n",
        "\n",
        "            for synapse in synapse_batch:\n",
        "                try:\n",
        "                    # Extract synapse image\n",
        "                    em_image = synapse.get_em_image()\n",
        "\n",
        "                    # Get the classification (from properties or derived features)\n",
        "                    if hasattr(synapse, 'is_excitatory'):\n",
        "                        is_excitatory = synapse.is_excitatory\n",
        "                    else:\n",
        "                        # Determine type from vesicle shape if available\n",
        "                        vesicle_props = synapse.get_vesicle_properties()\n",
        "                        # Excitatory synapses typically have round vesicles (higher circularity)\n",
        "                        # Inhibitory synapses typically have flattened vesicles (lower circularity)\n",
        "                        is_excitatory = vesicle_props.get('mean_circularity', 0.5) > 0.7\n",
        "\n",
        "                    # Preprocess the image\n",
        "                    processed_img = self._preprocess_image(em_image)\n",
        "\n",
        "                    # Only add valid images\n",
        "                    if processed_img is not None:\n",
        "                        images.append(processed_img)\n",
        "                        labels.append(\"excitatory\" if is_excitatory else \"inhibitory\")\n",
        "\n",
        "                    # Free memory\n",
        "                    del em_image\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing synapse: {e}\")\n",
        "\n",
        "            # Free memory after each batch\n",
        "            gc.collect()\n",
        "\n",
        "            # Check if we've reached the sample limit\n",
        "            if sample_limit and len(images) >= sample_limit:\n",
        "                images = images[:sample_limit]\n",
        "                labels = labels[:sample_limit]\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        # More comprehensive exception handling to catch both import and runtime errors\n",
        "        print(f\"Warning: MicronsCatalog not available or failed to initialize: {e}\")\n",
        "        print(\"Using placeholder data instead\")\n",
        "\n",
        "        # Placeholder for testing when microns_utils is not available\n",
        "        # Generate synthetic data for development/testing\n",
        "        n_samples = min(1000, sample_limit) if sample_limit else 1000\n",
        "\n",
        "        # Generate random images\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        for i in tqdm(range(n_samples), desc=\"Generating placeholder data\"):\n",
        "            # Generate a random grayscale image\n",
        "            img = np.random.rand(256, 256) * 255\n",
        "\n",
        "            # Create some \"vesicle-like\" structures\n",
        "            is_excitatory = np.random.random() > 0.4  # Slightly biased toward excitatory\n",
        "\n",
        "            if is_excitatory:\n",
        "                # Add round vesicle-like structures\n",
        "                for _ in range(np.random.randint(10, 30)):\n",
        "                    x, y = np.random.randint(20, 236, 2)\n",
        "                    radius = np.random.randint(3, 8)\n",
        "                    cv2.circle(img, (x, y), radius, 255, -1)\n",
        "            else:\n",
        "                # Add elongated vesicle-like structures\n",
        "                for _ in range(np.random.randint(10, 30)):\n",
        "                    x, y = np.random.randint(20, 236, 2)\n",
        "                    width = np.random.randint(2, 5)\n",
        "                    height = np.random.randint(5, 12)\n",
        "                    angle = np.random.randint(0, 180)\n",
        "                    box = ((x, y), (width, height), angle)\n",
        "                    cv2.ellipse(img, box, 255, -1)\n",
        "\n",
        "            # Add some noise\n",
        "            img += np.random.normal(0, 15, img.shape)\n",
        "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "            # Preprocess\n",
        "            processed_img = self._preprocess_image(img)\n",
        "\n",
        "            # Only add valid images\n",
        "            if processed_img is not None:\n",
        "                images.append(processed_img)\n",
        "                labels.append(\"excitatory\" if is_excitatory else \"inhibitory\")\n",
        "\n",
        "    # Check if we have any valid images\n",
        "    if not images:\n",
        "        raise ValueError(\"No valid synapse images could be loaded or generated\")\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    print(\"Converting to numpy arrays...\")\n",
        "    images = np.array(images)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Save the label encoder for later use\n",
        "    with open(os.path.join(self.output_path, 'label_encoder.pkl'), 'wb') as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "\n",
        "    print(f\"Loaded {len(images)} synapse images with shape {images.shape}\")\n",
        "    class_counts = pd.Series(labels).value_counts()\n",
        "    print(f\"Label distribution: {class_counts.to_dict()}\")\n",
        "\n",
        "    return images, encoded_labels\n",
        "\n",
        "def _preprocess_image(self, image):\n",
        "    \"\"\"\n",
        "    Preprocess a single EM image\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Input EM image\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Preprocessed image or None if preprocessing fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Handle empty or corrupt images\n",
        "        if image is None or image.size == 0:\n",
        "            # Return a blank image of the target size\n",
        "            return np.zeros((*self.image_size, 3), dtype=np.float32)\n",
        "\n",
        "        # Handle NaN or inf values\n",
        "        if np.isnan(image).any() or np.isinf(image).any():\n",
        "            # Replace NaN/inf with zeros\n",
        "            image = np.nan_to_num(image, nan=0.0, posinf=255.0, neginf=0.0)\n",
        "\n",
        "        # Convert to float32 for processing\n",
        "        image = image.astype(np.float32)\n",
        "\n",
        "        # Normalize to [0, 1] if needed\n",
        "        if image.max() > 1.0:\n",
        "            image = image / 255.0\n",
        "\n",
        "        # Check for valid dimensions before resize\n",
        "        if image.shape[0] == 0 or image.shape[1] == 0:\n",
        "            return np.zeros((*self.image_size, 3), dtype=np.float32)\n",
        "\n",
        "        # Resize to target dimensions\n",
        "        image = cv2.resize(image, self.image_size)\n",
        "\n",
        "        # Apply contrast enhancement\n",
        "        image = self._enhance_contrast(image)\n",
        "\n",
        "        # Add channel dimension if needed\n",
        "        if len(image.shape) == 2:\n",
        "            image = np.expand_dims(image, axis=-1)\n",
        "            # Convert grayscale to 3 channels for compatibility with pre-trained models\n",
        "            image = np.repeat(image, 3, axis=-1)\n",
        "\n",
        "        # Ensure the image is properly scaled\n",
        "        if image.max() > 1.0:\n",
        "            image = image / image.max()\n",
        "\n",
        "        # Final check for valid output\n",
        "        if np.isnan(image).any() or np.isinf(image).any():\n",
        "            return np.zeros((*self.image_size, 3), dtype=np.float32)\n",
        "\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in preprocessing image: {e}\")\n",
        "        # Return a blank image in case of error\n",
        "        return np.zeros((*self.image_size, 3), dtype=np.float32)\n",
        "\n",
        "def _enhance_contrast(self, image):\n",
        "    \"\"\"\n",
        "    Enhance contrast in the EM image\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Input image\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Contrast-enhanced image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Make a copy to avoid modifying the original\n",
        "        img = image.copy()\n",
        "\n",
        "        # Handle NaN or inf values if any\n",
        "        if np.isnan(img).any() or np.isinf(img).any():\n",
        "            img = np.nan_to_num(img, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "\n",
        "        # Make sure image is in the proper range for CLAHE\n",
        "        if img.min() < 0 or img.max() > 1.0:\n",
        "            img = np.clip(img, 0.0, 1.0)\n",
        "\n",
        "        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "        if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[2] == 1):\n",
        "            # Convert to uint8 for CLAHE\n",
        "            img_uint8 = (img * 255).astype(np.uint8)\n",
        "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "            img = clahe.apply(img_uint8) / 255.0\n",
        "        else:\n",
        "            # For RGB images, apply CLAHE to the luminance channel\n",
        "            img_uint8 = (img * 255).astype(np.uint8)\n",
        "            try:\n",
        "                img_lab = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2LAB)\n",
        "                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "                img_lab[..., 0] = clahe.apply(img_lab[..., 0])\n",
        "                img = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB) / 255.0\n",
        "            except cv2.error:\n",
        "                # If color conversion fails, revert to original image\n",
        "                img = image.copy()\n",
        "\n",
        "        # Apply Gaussian blur to reduce noise\n",
        "        img = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "\n",
        "        # Normalize to [0, 1] range safely (avoid division by zero)\n",
        "        min_val = img.min()\n",
        "        max_val = img.max()\n",
        "        if max_val > min_val:\n",
        "            img = (img - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            # If all values are the same, return a normalized constant image\n",
        "            img = np.zeros_like(img)\n",
        "\n",
        "        return img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error enhancing contrast: {e}\")\n",
        "        return image  # Return original image if enhancement fails"
      ],
      "metadata": {
        "id": "HxCF0Dp_U22P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_microns_data(self, sample_limit=None):\n",
        "    \"\"\"\n",
        "    Load and preprocess data from the Microns Explorer dataset\n",
        "\n",
        "    Args:\n",
        "        sample_limit (int, optional): Limit the number of samples to load for testing\n",
        "\n",
        "    Returns:\n",
        "        tuple: Processed images and their corresponding labels\n",
        "    \"\"\"\n",
        "    print(\"Loading Microns Explorer dataset...\")\n",
        "\n",
        "    # Initialize the microns catalog\n",
        "    try:\n",
        "        # Proper Microns dataset access\n",
        "        catalog = MicronsCatalog(self.data_path)\n",
        "        synapse_collection = catalog.get_collection('synapses')\n",
        "\n",
        "        # Get synapses with their labels\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        # Load synapse data in batches to reduce memory usage\n",
        "        batch_size = 100  # Process 100 synapses at a time\n",
        "        for i in tqdm(range(0, synapse_collection.count(), batch_size), desc=\"Loading synapse batches\"):\n",
        "            # Get a batch of synapses\n",
        "            synapse_batch = synapse_collection.get_items(i, min(i + batch_size, synapse_collection.count()))\n",
        "\n",
        "            for synapse in synapse_batch:\n",
        "                try:\n",
        "                    # Extract synapse image\n",
        "                    em_image = synapse.get_em_image()\n",
        "\n",
        "                    # Get the classification (from properties or derived features)\n",
        "                    if hasattr(synapse, 'is_excitatory'):\n",
        "                        is_excitatory = synapse.is_excitatory\n",
        "                    else:\n",
        "                        # Determine type from vesicle shape if available\n",
        "                        vesicle_props = synapse.get_vesicle_properties()\n",
        "                        # Excitatory synapses typically have round vesicles (higher circularity)\n",
        "                        # Inhibitory synapses typically have flattened vesicles (lower circularity)\n",
        "                        is_excitatory = vesicle_props.get('mean_circularity', 0.5) > 0.7\n",
        "\n",
        "                    # Preprocess the image\n",
        "                    processed_img = self._preprocess_image(em_image)\n",
        "\n",
        "                    images.append(processed_img)\n",
        "                    labels.append(\"excitatory\" if is_excitatory else \"inhibitory\")\n",
        "\n",
        "                    # Free memory\n",
        "                    del em_image\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing synapse: {e}\")\n",
        "\n",
        "            # Free memory after each batch\n",
        "            gc.collect()\n",
        "\n",
        "            # Check if we've reached the sample limit\n",
        "            if sample_limit and len(images) >= sample_limit:\n",
        "                images = images[:sample_limit]\n",
        "                labels = labels[:sample_limit]\n",
        "                break\n",
        "\n",
        "    except Exception as e:  # Change to catch all exceptions, not just ImportError/NameError\n",
        "        print(f\"Warning: MicronsCatalog not available or failed: {e}\")\n",
        "        print(\"Using placeholder data instead\")\n",
        "        # Placeholder for testing when microns_utils is not available\n",
        "        # Generate synthetic data for development/testing\n",
        "        n_samples = min(1000, sample_limit) if sample_limit else 1000\n",
        "\n",
        "        # Generate random images\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        for i in tqdm(range(n_samples), desc=\"Generating placeholder data\"):\n",
        "            # Generate a random grayscale image\n",
        "            img = np.random.rand(256, 256) * 255\n",
        "\n",
        "            # Create some \"vesicle-like\" structures\n",
        "            is_excitatory = np.random.random() > 0.4  # Slightly biased toward excitatory\n",
        "\n",
        "            if is_excitatory:\n",
        "                # Add round vesicle-like structures\n",
        "                for _ in range(np.random.randint(10, 30)):\n",
        "                    x, y = np.random.randint(20, 236, 2)\n",
        "                    radius = np.random.randint(3, 8)\n",
        "                    cv2.circle(img, (x, y), radius, 255, -1)\n",
        "            else:\n",
        "                # Add elongated vesicle-like structures\n",
        "                for _ in range(np.random.randint(10, 30)):\n",
        "                    x, y = np.random.randint(20, 236, 2)\n",
        "                    width = np.random.randint(2, 5)\n",
        "                    height = np.random.randint(5, 12)\n",
        "                    angle = np.random.randint(0, 180)\n",
        "                    box = ((x, y), (width, height), angle)\n",
        "                    cv2.ellipse(img, box, 255, -1)\n",
        "\n",
        "            # Add some noise\n",
        "            img += np.random.normal(0, 15, img.shape)\n",
        "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "            # Preprocess\n",
        "            processed_img = self._preprocess_image(img)\n",
        "\n",
        "            images.append(processed_img)\n",
        "            labels.append(\"excitatory\" if is_excitatory else \"inhibitory\")\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    print(\"Converting to numpy arrays...\")\n",
        "    images = np.array(images)\n",
        "\n",
        "    # Check if we have any images to process\n",
        "    if len(images) == 0:\n",
        "        raise ValueError(\"No valid images could be loaded or generated\")\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Save the label encoder for later use\n",
        "    with open(os.path.join(self.output_path, 'label_encoder.pkl'), 'wb') as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "\n",
        "    print(f\"Loaded {len(images)} synapse images with shape {images.shape}\")\n",
        "    class_counts = pd.Series(labels).value_counts()\n",
        "    print(f\"Label distribution: {class_counts.to_dict()}\")\n",
        "\n",
        "    return images, encoded_labels\n",
        "\n",
        "def _preprocess_image(self, image):\n",
        "    \"\"\"\n",
        "    Preprocess a single EM image\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Input EM image\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Preprocessed image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Handle empty or corrupt images\n",
        "        if image is None or image.size == 0:\n",
        "            # Return a blank image of the target size\n",
        "            return np.zeros((*self.image_size, 3), dtype=np.float32)\n",
        "\n",
        "        # Handle NaN/Inf values\n",
        "        if np.isnan(image).any() or np.isinf(image).any():\n",
        "            image = np.nan_to_num(image)\n",
        "\n",
        "        # Convert to float32 for processing\n",
        "        image = image.astype(np.float32)\n",
        "\n",
        "        # Normalize to [0, 1] if needed\n",
        "        if image.max() > 1.0:\n",
        "            image = image / 255.0\n",
        "\n",
        "        # Resize to target dimensions (check for valid image size)\n",
        "        if image.shape[0] > 0 and image.shape[1] > 0:\n",
        "            image = cv2.resize(image, self.image_size)\n",
        "        else:\n",
        "            return np.zeros((*self.image_size, 3), dtype=np.float32)\n",
        "\n",
        "        # Apply contrast enhancement\n",
        "        image = self._enhance_contrast(image)\n",
        "\n",
        "        # Add channel dimension if needed\n",
        "        if len(image.shape) == 2:\n",
        "            image = np.expand_dims(image, axis=-1)\n",
        "            # Convert grayscale to 3 channels for compatibility with pre-trained models\n",
        "            image = np.repeat(image, 3, axis=-1)\n",
        "\n",
        "        # Ensure the image is properly scaled\n",
        "        if image.max() > 1.0:\n",
        "            image = image / image.max()\n",
        "\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in preprocessing image: {e}\")\n",
        "        # Return a blank image in case of error\n",
        "        return np.zeros((*self.image_size, 3), dtype=np.float32)\n",
        "\n",
        "def _enhance_contrast(self, image):\n",
        "    \"\"\"\n",
        "    Enhance contrast in the EM image\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Input image\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Contrast-enhanced image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Make a copy to avoid modifying the original\n",
        "        img = image.copy()\n",
        "\n",
        "        # Ensure image doesn't contain NaN/Inf values\n",
        "        if np.isnan(img).any() or np.isinf(img).any():\n",
        "            img = np.nan_to_num(img)\n",
        "\n",
        "        # Ensure valid range for processing\n",
        "        img = np.clip(img, 0.0, 1.0)\n",
        "\n",
        "        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "        if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[2] == 1):\n",
        "            # Convert to uint8 for CLAHE\n",
        "            img_uint8 = (img * 255).astype(np.uint8)\n",
        "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "            img = clahe.apply(img_uint8) / 255.0\n",
        "        else:\n",
        "            # For RGB images, apply CLAHE to the luminance channel\n",
        "            img_uint8 = (img * 255).astype(np.uint8)\n",
        "            try:\n",
        "                img_lab = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2LAB)\n",
        "                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "                img_lab[..., 0] = clahe.apply(img_lab[..., 0])\n",
        "                img = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB) / 255.0\n",
        "            except cv2.error:\n",
        "                # If conversion fails, use original image\n",
        "                print(\"Color conversion failed, using original image\")\n",
        "                return image\n",
        "\n",
        "        # Apply Gaussian blur to reduce noise\n",
        "        img = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "\n",
        "        # Normalize to [0, 1] range with protection against division by zero\n",
        "        denom = img.max() - img.min()\n",
        "        if denom > 1e-8:  # Only normalize if there's a meaningful range\n",
        "            img = (img - img.min()) / denom\n",
        "        else:\n",
        "            # If all values are essentially the same, return the original image\n",
        "            return image\n",
        "\n",
        "        # Final check to ensure output is in valid range\n",
        "        img = np.clip(img, 0.0, 1.0)\n",
        "\n",
        "        return img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error enhancing contrast: {e}\")\n",
        "        return image  # Return original image if enhancement fails"
      ],
      "metadata": {
        "id": "OYiwFWjHYKis"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_synapse_regions(self, em_volume, segmentation_mask, max_synapses=500):\n",
        "        \"\"\"\n",
        "        Extract individual synapse regions from a larger EM volume using segmentation masks\n",
        "\n",
        "        Args:\n",
        "            em_volume (numpy.ndarray): 3D EM volume\n",
        "            segmentation_mask (numpy.ndarray): Synapse segmentation mask\n",
        "            max_synapses (int): Maximum number of synapses to extract to avoid memory issues\n",
        "\n",
        "        Returns:\n",
        "            list: List of individual synapse patches\n",
        "        \"\"\"\n",
        "        print(\"Extracting synapse regions...\")\n",
        "\n",
        "        try:\n",
        "            # Find connected components in the segmentation mask\n",
        "            # Use a memory-efficient approach for large volumes\n",
        "            if segmentation_mask.size > 1e9:  # If mask is larger than ~1GB\n",
        "                print(\"Large volume detected, using memory-efficient processing...\")\n",
        "                # Process in slices along z-axis\n",
        "                all_patches = []\n",
        "                z_slices = np.array_split(np.arange(segmentation_mask.shape[0]), 10)\n",
        "\n",
        "                for z_slice in tqdm(z_slices, desc=\"Processing volume slices\"):\n",
        "                    slice_mask = segmentation_mask[z_slice.min():z_slice.max()+1]\n",
        "                    slice_volume = em_volume[z_slice.min():z_slice.max()+1]\n",
        "\n",
        "                    # Find connected components in this slice\n",
        "                    labeled_slice, num_features = ndi.label(slice_mask)\n",
        "\n",
        "                    # Extract patches from this slice\n",
        "                    for i in range(1, min(num_features+1, max_synapses//len(z_slices)+1)):\n",
        "                        coords = np.where(labeled_slice == i)\n",
        "                        if len(coords[0]) == 0:\n",
        "                            continue\n",
        "\n",
        "                        z_min, z_max = np.min(coords[0]), np.max(coords[0])\n",
        "                        y_min, y_max = np.min(coords[1]), np.max(coords[1])\n",
        "                        x_min, x_max = np.min(coords[2]), np.max(coords[2])\n",
        "\n",
        "                        # Add margin\n",
        "                        margin = 10\n",
        "                        z_min = max(0, z_min - margin)\n",
        "                        y_min = max(0, y_min - margin)\n",
        "                        x_min = max(0, x_min - margin)\n",
        "                        z_max = min(slice_volume.shape[0], z_max + margin)\n",
        "                        y_max = min(slice_volume.shape[1], y_max + margin)\n",
        "                        x_max = min(slice_volume.shape[2], x_max + margin)\n",
        "\n",
        "                        # Extract patch\n",
        "                        patch = slice_volume[z_min:z_max, y_min:y_max, x_min:x_max]\n",
        "\n",
        "                        # Create a 2D representation\n",
        "                        if patch.shape[0] > 0:\n",
        "                            # Take middle slice or max projection\n",
        "                            if patch.shape[0] > 1:\n",
        "                                middle_slice = patch[patch.shape[0] // 2]\n",
        "                            else:\n",
        "                                middle_slice = patch[0]\n",
        "\n",
        "                            all_patches.append(middle_slice)\n",
        "\n",
        "                    # Free memory\n",
        "                    del labeled_slice, slice_mask, slice_volume\n",
        "                    gc.collect()\n",
        "\n",
        "                return all_patches[:max_synapses]  # Limit total patches\n",
        "\n",
        "            else:\n",
        "                # For smaller volumes, use standard approach\n",
        "                labeled_mask, num_features = ndi.label(segmentation_mask)\n",
        "\n",
        "                synapse_patches = []\n",
        "                for i in tqdm(range(1, min(num_features + 1, max_synapses + 1)),\n",
        "                              desc=\"Extracting patches\"):\n",
        "                    # Get bounding box for each synapse\n",
        "                    coords = np.where(labeled_mask == i)\n",
        "                    if len(coords[0]) == 0:\n",
        "                        continue\n",
        "\n",
        "                    z_min, z_max = np.min(coords[0]), np.max(coords[0])\n",
        "                    y_min, y_max = np.min(coords[1]), np.max(coords[1])\n",
        "                    x_min, x_max = np.min(coords[2]), np.max(coords[2])\n",
        "\n",
        "                    # Add margin\n",
        "                    margin = 10\n",
        "                    z_min = max(0, z_min - margin)\n",
        "                    y_min = max(0, y_min - margin)\n",
        "                    x_min = max(0, x_min - margin)\n",
        "                    z_max = min(em_volume.shape[0], z_max + margin)\n",
        "                    y_max = min(em_volume.shape[1], y_max + margin)\n",
        "                    x_max = min(em_volume.shape[2], x_max + margin)\n",
        "\n",
        "                    # Extract patch\n",
        "                    patch = em_volume[z_min:z_max, y_min:y_max, x_min:x_max]\n",
        "\n",
        "                    # Create a 2D representation (max projection or middle slice)\n",
        "                    if patch.shape[0] > 0:\n",
        "                        # Take middle slice\n",
        "                        middle_slice = patch[patch.shape[0] // 2]\n",
        "                        synapse_patches.append(middle_slice)\n",
        "\n",
        "                return synapse_patches\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting synapse regions: {e}\")\n",
        "            return []"
      ],
      "metadata": {
        "id": "4_AnQY7xSB9B"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet_segmentation_model(self, input_shape=(512, 512, 1)):\n",
        "        \"\"\"\n",
        "        Build a U-Net model for synapse segmentation\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): Shape of input images\n",
        "\n",
        "        Returns:\n",
        "            tensorflow.keras.Model: U-Net segmentation model\n",
        "        \"\"\"\n",
        "        # Memory-optimized U-Net with dropout and batch normalization\n",
        "        inputs = tf.keras.Input(input_shape)\n",
        "\n",
        "        # Encoder (Downsampling path) with batch normalization and dropout\n",
        "        c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "        c1 = layers.BatchNormalization()(c1)\n",
        "        c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "        c1 = layers.BatchNormalization()(c1)\n",
        "        p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "        p1 = layers.Dropout(0.1)(p1)\n",
        "\n",
        "        c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "        c2 = layers.BatchNormalization()(c2)\n",
        "        c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "        c2 = layers.BatchNormalization()(c2)\n",
        "        p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "        p2 = layers.Dropout(0.2)(p2)\n",
        "\n",
        "        c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "        c3 = layers.BatchNormalization()(c3)\n",
        "        c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "        c3 = layers.BatchNormalization()(c3)\n",
        "        p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "        p3 = layers.Dropout(0.3)(p3)\n",
        "\n",
        "        c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "        c4 = layers.BatchNormalization()(c4)\n",
        "        c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "        c4 = layers.BatchNormalization()(c4)\n",
        "        p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "        p4 = layers.Dropout(0.4)(p4)\n",
        "\n",
        "        # Bridge\n",
        "        c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "        c5 = layers.BatchNormalization()(c5)\n",
        "        c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "        c5 = layers.BatchNormalization()(c5)\n",
        "        c5 = layers.Dropout(0.5)(c5)\n",
        "\n",
        "        # Decoder (Upsampling path)\n",
        "        u6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "        u6 = layers.concatenate([u6, c4])\n",
        "        u6 = layers.Dropout(0.4)(u6)\n",
        "        c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "        c6 = layers.BatchNormalization()(c6)\n",
        "        c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "        c6 = layers.BatchNormalization()(c6)\n",
        "\n",
        "        u7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "        u7 = layers.concatenate([u7, c3])\n",
        "        u7 = layers.Dropout(0.3)(u7)\n",
        "        c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "        c7 = layers.BatchNormalization()(c7)\n",
        "        c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "        c7 = layers.BatchNormalization()(c7)\n",
        "\n",
        "        u8 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "        u8 = layers.concatenate([u8, c2])\n",
        "        u8 = layers.Dropout(0.2)(u8)\n",
        "        c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "        c8 = layers.BatchNormalization()(c8)\n",
        "        c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "        c8 = layers.BatchNormalization()(c8)\n",
        "\n",
        "        u9 = layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "        u9 = layers.concatenate([u9, c1], axis=3)\n",
        "        u9 = layers.Dropout(0.1)(u9)\n",
        "        c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "        c9 = layers.BatchNormalization()(c9)\n",
        "        c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "        c9 = layers.BatchNormalization()(c9)\n",
        "\n",
        "        outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "        model = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "        # Use dice coefficient loss for better segmentation\n",
        "        def dice_coef(y_true, y_pred, smooth=1.0):\n",
        "            y_true_f = K.flatten(y_true)\n",
        "            y_pred_f = K.flatten(y_pred)\n",
        "            intersection = K.sum(y_true_f * y_pred_f)\n",
        "            return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "        def dice_coef_loss(y_true, y_pred):\n",
        "            return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss=dice_coef_loss,\n",
        "            metrics=['accuracy', dice_coef]\n",
        "        )\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "m85vPnKZSGmm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classification_model(self, num_classes=2, use_pretrained=True):\n",
        "        \"\"\"\n",
        "        Build the synapse classification model\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of classes to predict\n",
        "            use_pretrained (bool): Whether to use a pre-trained backbone model\n",
        "\n",
        "        Returns:\n",
        "            tensorflow.keras.Model: Classification model\n",
        "        \"\"\"\n",
        "        if use_pretrained:\n",
        "            # Use EfficientNetV2S which works better for grayscale/EM images\n",
        "            base_model = applications.EfficientNetV2S(\n",
        "                include_top=False,\n",
        "                weights='imagenet',\n",
        "                input_shape=(*self.image_size, 3),\n",
        "                include_preprocessing=False  # We do our own preprocessing\n",
        "            )\n",
        "\n",
        "            # Freeze the base model initially\n",
        "            base_model.trainable = False\n",
        "\n",
        "            # Create new model on top\n",
        "            inputs = layers.Input(shape=(*self.image_size, 3))\n",
        "\n",
        "            # Preprocessing specific to EM images\n",
        "            x = inputs\n",
        "\n",
        "            # Pass through the base model\n",
        "            x = base_model(x, training=False)\n",
        "\n",
        "            # Add global average pooling layer\n",
        "            x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "            # Add dropout and dense layers\n",
        "            x = layers.Dropout(0.4)(x)\n",
        "            x = layers.Dense(512, activation='relu')(x)\n",
        "            x = layers.BatchNormalization()(x)\n",
        "            x = layers.Dropout(0.5)(x)\n",
        "            x = layers.Dense(128, activation='relu')(x)\n",
        "            x = layers.BatchNormalization()(x)\n",
        "\n",
        "            # Add the final dense layer with softmax activation\n",
        "            outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "            # Create the model\n",
        "            model = models.Model(inputs, outputs)\n",
        "\n",
        "        else:\n",
        "            # Create a custom CNN from scratch, optimized for EM images\n",
        "            model = models.Sequential([\n",
        "                # Initial layer to handle grayscale/EM images\n",
        "                layers.Conv2D(32, (5, 5), padding='same', input_shape=(*self.image_size, 3)),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.LeakyReLU(alpha=0.1),\n",
        "                layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "                # Second block\n",
        "                layers.Conv2D(64, (3, 3), padding='same'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.LeakyReLU(alpha=0.1),\n",
        "                layers.Conv2D(64, (3, 3), padding='same'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.LeakyReLU(alpha=0.1),\n",
        "                layers.MaxPooling2D((2, 2)),\n",
        "                layers.Dropout(0.2),\n",
        "\n",
        "                # Third block\n",
        "                layers.Conv2D(128, (3, 3), padding='same'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.LeakyReLU(alpha=0.1),\n",
        "                layers.Conv2D(128, (3, 3), padding='same'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.LeakyReLU(alpha=0.1),\n",
        "                layers.MaxPooling2D((2, 2)),\n",
        "                layers.Dropout(0.3),\n",
        "\n",
        "                # Fourth block\n",
        "                layers.Conv2D(256, (3, 3), padding='same'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.LeakyReLU(alpha=0.1),\n",
        "                layers.Conv2D(256, (3, 3), padding='same'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.LeakyReLU(alpha=0.1),\n",
        "                layers.MaxPooling2D((2, 2)),\n",
        "                layers.Dropout(0.4),\n",
        "\n",
        "                # Final classification layers\n",
        "                layers.Flatten(),\n",
        "                layers.Dense(512, activation='relu'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.Dropout(0.5),\n",
        "                layers.Dense(128, activation='relu'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.Dropout(0.5),\n",
        "                layers.Dense(num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "        # Compile the model with focal loss for better handling of class imbalance\n",
        "        def focal_loss(gamma=2.0, alpha=0.25):\n",
        "            def focal_loss_fn(y_true, y_pred):\n",
        "                # Convert labels to one-hot encoding\n",
        "                y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n",
        "\n",
        "                # Calculate focal loss\n",
        "                epsilon = 1e-7\n",
        "                y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
        "\n",
        "                # Calculate cross entropy\n",
        "                cross_entropy = -y_true_one_hot * tf.math.log(y_pred)\n",
        "\n",
        "                # Calculate focal term\n",
        "                focal_term = (1 - y_pred) ** gamma\n",
        "\n",
        "                # Calculate final focal loss\n",
        "                loss = alpha * focal_term * cross_entropy\n",
        "\n",
        "                return tf.reduce_sum(loss, axis=-1)\n",
        "\n",
        "            return focal_loss_fn\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss=focal_loss(gamma=2.0),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "2BJiSVrvSKT1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(self, X, y, validation_split=0.2, epochs=50, fine_tune=True):\n",
        "    \"\"\"\n",
        "    Train the classification model with advanced techniques\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Input images\n",
        "        y (numpy.ndarray): Target labels\n",
        "        validation_split (float): Fraction of data to use for validation\n",
        "        epochs (int): Number of training epochs\n",
        "        fine_tune (bool): Whether to fine-tune the pre-trained model\n",
        "\n",
        "    Returns:\n",
        "        dict: Training history\n",
        "    \"\"\"\n",
        "    # Use stratified k-fold to ensure balanced validation sets\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_idx = next(skf.split(X, y))\n",
        "    train_idx, val_idx = fold_idx\n",
        "\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
        "\n",
        "    # Calculate class weights to handle imbalanced data\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "    # Create data generators with augmentation for training\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=180,            # More rotation for EM images\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,                # More zoom variation\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,            # EM has no natural orientation\n",
        "        brightness_range=(0.8, 1.2),   # Vary brightness\n",
        "        fill_mode='reflect',\n",
        "        preprocessing_function=lambda x: x + np.random.normal(0, 0.05, x.shape)  # Add noise\n",
        "    )\n",
        "\n",
        "    # Use a simple validation generator without augmentation\n",
        "    val_datagen = ImageDataGenerator()\n",
        "\n",
        "    train_generator = train_datagen.flow(\n",
        "        X_train, y_train,\n",
        "        batch_size=self.batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow(\n",
        "        X_val, y_val,\n",
        "        batch_size=self.batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Memory optimization - clear unnecessary variables\n",
        "    gc.collect()\n",
        "\n",
        "    # Create callbacks with improved settings\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,               # More patience for EM images\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        filepath=os.path.join(self.output_path, 'best_model.h5'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,                # More aggressive LR reduction\n",
        "        patience=7,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Add TensorBoard callback for better monitoring\n",
        "    tensorboard = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=os.path.join(self.output_path, 'logs'),\n",
        "        histogram_freq=1,\n",
        "        write_graph=True,\n",
        "        update_freq='epoch'\n",
        "    )\n",
        "\n",
        "    callbacks = [early_stopping, model_checkpoint, reduce_lr, tensorboard]\n",
        "\n",
        "    # Build and train the model with error handling\n",
        "    try:\n",
        "        if self.model is None:\n",
        "            self.model = self.build_classification_model()\n",
        "            print(f\"Model successfully built with {self.model.count_params():,} parameters\")\n",
        "            self.model.summary()\n",
        "\n",
        "        # Set up training with memory optimization\n",
        "        steps_per_epoch = len(X_train) // self.batch_size\n",
        "        validation_steps = len(X_val) // self.batch_size\n",
        "\n",
        "        # Train the model with mixed precision for performance\n",
        "        # Note: Remove mixed_precision if not supported by hardware\n",
        "        try:\n",
        "            from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "            policy = mixed_precision.Policy('mixed_float16')\n",
        "            mixed_precision.set_global_policy(policy)\n",
        "            print(\"Mixed precision training enabled\")\n",
        "        except:\n",
        "            print(\"Mixed precision not available, using default precision\")\n",
        "\n",
        "        print(f\"Training the model for {epochs} epochs...\")\n",
        "        history = self.model.fit(\n",
        "            train_generator,\n",
        "            validation_data=val_generator,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weight_dict,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            validation_steps=validation_steps,\n",
        "            workers=4,                  # Parallel processing\n",
        "            use_multiprocessing=True,   # Enable multiprocessing\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Track metrics for later analysis\n",
        "        for i, (loss, acc, val_loss, val_acc) in enumerate(zip(\n",
        "            history.history['loss'],\n",
        "            history.history['accuracy'],\n",
        "            history.history['val_loss'],\n",
        "            history.history['val_accuracy']\n",
        "        )):\n",
        "            print(f\"Epoch {i+1}/{epochs} - loss: {loss:.4f} - accuracy: {acc:.4f} - val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f}\")\n",
        "\n",
        "        # Save training metrics\n",
        "        self.metrics_history['train_loss'].extend(history.history['loss'])\n",
        "        self.metrics_history['val_loss'].extend(history.history['val_loss'])\n",
        "        self.metrics_history['train_acc'].extend(history.history['accuracy'])\n",
        "        self.metrics_history['val_acc'].extend(history.history['val_accuracy'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model training: {e}\")\n",
        "        # Try to recover by creating a simpler model\n",
        "        print(\"Attempting to train with a simpler model architecture...\")\n",
        "        self.model = self.build_classification_model(use_pretrained=False)\n",
        "        history = self.model.fit(\n",
        "            train_generator,\n",
        "            validation_data=val_generator,\n",
        "            epochs=min(20, epochs),  # Shorter training for recovery\n",
        "            callbacks=[early_stopping, model_checkpoint],\n",
        "            class_weight=class_weight_dict\n",
        "        )\n",
        "\n",
        "    # Fine-tune the model if specified\n",
        "    if fine_tune and hasattr(self.model, 'layers') and len(self.model.layers) > 0 and hasattr(self.model.layers[0], 'trainable'):\n",
        "        try:\n",
        "            print(\"Fine-tuning the model...\")\n",
        "\n",
        "            # Check if the model can be fine-tuned (has trainable layers)\n",
        "            trainable_params_before = sum([tf.size(w).numpy() for w in self.model.trainable_weights])\n",
        "\n",
        "            # Gradually unfreeze layers from top to bottom for better transfer learning\n",
        "            if isinstance(self.model.layers[0], tf.keras.models.Model):  # For EfficientNet or other pre-trained models\n",
        "                base_model = self.model.layers[0]\n",
        "\n",
        "                # Safety check for base model layers\n",
        "                if hasattr(base_model, 'layers') and len(base_model.layers) > 0:\n",
        "                    # Calculate number of layers to unfreeze (30% of total)\n",
        "                    num_layers_to_unfreeze = max(1, int(len(base_model.layers) * 0.3))\n",
        "\n",
        "                    # Unfreeze the top layers first (last 30%)\n",
        "                    for layer in base_model.layers[-num_layers_to_unfreeze:]:\n",
        "                        if hasattr(layer, 'trainable'):\n",
        "                            layer.trainable = True\n",
        "\n",
        "                    print(f\"Fine-tuning top 30% of base model layers: {num_layers_to_unfreeze} layers unfrozen\")\n",
        "                else:\n",
        "                    print(\"Base model has no layers, setting entire model to trainable\")\n",
        "                    self.model.trainable = True\n",
        "            else:\n",
        "                # For custom model, unfreeze all layers\n",
        "                self.model.trainable = True\n",
        "                print(\"Fine-tuning all layers\")\n",
        "\n",
        "            # Verify that some parameters are actually trainable now\n",
        "            trainable_params_after = sum([tf.size(w).numpy() for w in self.model.trainable_weights])\n",
        "            if trainable_params_after <= trainable_params_before:\n",
        "                print(f\"Warning: No additional parameters were made trainable ({trainable_params_before} -> {trainable_params_after})\")\n",
        "                if trainable_params_after == 0:\n",
        "                    print(\"No trainable parameters found, skipping fine-tuning\")\n",
        "                    raise ValueError(\"Model has no trainable parameters\")\n",
        "\n",
        "            # Custom learning rate scheduler for fine-tuning with decay\n",
        "            def lr_scheduler(epoch, lr):\n",
        "                if epoch < 5:\n",
        "                    return lr  # Keep initial learning rate for first 5 epochs\n",
        "                else:\n",
        "                    # Exponential decay after 5 epochs\n",
        "                    return lr * tf.math.exp(-0.1)\n",
        "\n",
        "            lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "            # Recompile with a lower learning rate and appropriate loss function\n",
        "            try:\n",
        "                if hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'learning_rate'):\n",
        "                    # Extract current learning rate safely\n",
        "                    try:\n",
        "                        current_lr = float(self.model.optimizer.learning_rate.numpy())\n",
        "                    except (AttributeError, ValueError, TypeError):\n",
        "                        current_lr = 0.001  # Default Adam learning rate\n",
        "\n",
        "                    # Reduce learning rate for fine-tuning\n",
        "                    new_lr = current_lr * 0.1\n",
        "                else:\n",
        "                    # Default fine-tuning learning rate\n",
        "                    new_lr = 1e-5\n",
        "            except Exception as lr_error:\n",
        "                # Handle any unexpected errors when accessing the optimizer\n",
        "                print(f\"Warning: Could not determine current learning rate: {lr_error}\")\n",
        "                new_lr = 1e-5\n",
        "\n",
        "            # Ensure learning rate is in a reasonable range\n",
        "            new_lr = max(1e-7, min(new_lr, 1e-3))  # Between 1e-7 and 1e-3\n",
        "            print(f\"Fine-tuning with learning rate: {new_lr}\")\n",
        "\n",
        "            # Choose appropriate loss function based on model output\n",
        "            try:\n",
        "                # Check output shape to determine appropriate loss function\n",
        "                if hasattr(self.model, 'output_shape') and isinstance(self.model.output_shape, tuple):\n",
        "                    if self.model.output_shape[-1] > 2:  # Multi-class\n",
        "                        loss_fn = 'sparse_categorical_crossentropy'\n",
        "                    else:  # Binary\n",
        "                        loss_fn = 'binary_crossentropy' if self.model.output_shape[-1] == 1 else 'sparse_categorical_crossentropy'\n",
        "                else:\n",
        "                    # Fallback if output_shape is not available\n",
        "                    loss_fn = 'sparse_categorical_crossentropy'\n",
        "                    print(\"Warning: Could not determine model output shape, using sparse_categorical_crossentropy\")\n",
        "            except Exception as shape_error:\n",
        "                # Fallback for any errors\n",
        "                print(f\"Warning: Error determining model output shape: {shape_error}\")\n",
        "                loss_fn = 'sparse_categorical_crossentropy'\n",
        "\n",
        "            # Compile model with appropriate settings\n",
        "            try:\n",
        "                self.model.compile(\n",
        "                    optimizer=tf.keras.optimizers.Adam(learning_rate=new_lr),\n",
        "                    loss=loss_fn,\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "            except Exception as compile_error:\n",
        "                print(f\"Error compiling model for fine-tuning: {compile_error}\")\n",
        "                print(\"Attempting to compile with default parameters\")\n",
        "                self.model.compile(\n",
        "                    optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "            # Calculate fine-tuning epochs with safeguards\n",
        "            if isinstance(epochs, int) and epochs > 0:\n",
        "                fine_tune_epochs = min(20, max(5, epochs // 2))  # Between 5 and 20\n",
        "            else:\n",
        "                fine_tune_epochs = 10  # Default if epochs is invalid\n",
        "\n",
        "            # Update callbacks with safeguards\n",
        "            if isinstance(callbacks, list):\n",
        "                ft_callbacks = callbacks.copy()  # Make a copy to avoid modifying the original\n",
        "                ft_callbacks.append(lr_callback)\n",
        "            else:\n",
        "                # Fallback if callbacks is not a list\n",
        "                print(\"Warning: callbacks is not a list, using default callbacks for fine-tuning\")\n",
        "                ft_callbacks = [\n",
        "                    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "                    lr_callback\n",
        "                ]\n",
        "\n",
        "            # Determine initial epoch safely\n",
        "            if hasattr(history, 'history') and isinstance(history.history, dict) and 'loss' in history.history:\n",
        "                initial_epoch = len(history.history['loss'])\n",
        "            else:\n",
        "                initial_epoch = 0\n",
        "                print(\"Warning: No training history found, starting fine-tuning from epoch 0\")\n",
        "\n",
        "            # Fine-tuning with proper error handling\n",
        "            print(f\"Starting fine-tuning for {fine_tune_epochs} epochs (from epoch {initial_epoch})\")\n",
        "            try:\n",
        "                fine_tune_history = self.model.fit(\n",
        "                    train_generator,\n",
        "                    validation_data=val_generator,\n",
        "                    epochs=initial_epoch + fine_tune_epochs,\n",
        "                    initial_epoch=initial_epoch,\n",
        "                    callbacks=ft_callbacks,\n",
        "                    class_weight=class_weight_dict if isinstance(class_weight_dict, dict) else None,\n",
        "                    verbose=1\n",
        "                )\n",
        "\n",
        "                # Combine histories safely\n",
        "                if (hasattr(history, 'history') and isinstance(history.history, dict) and\n",
        "                    hasattr(fine_tune_history, 'history') and isinstance(fine_tune_history.history, dict)):\n",
        "\n",
        "                    # Check each key before extending\n",
        "                    for k in list(history.history.keys()):\n",
        "                        if k in fine_tune_history.history:\n",
        "                            history.history[k].extend(fine_tune_history.history[k])\n",
        "\n",
        "                    # Update metrics history with safeguards\n",
        "                    if hasattr(self, 'metrics_history') and isinstance(self.metrics_history, dict):\n",
        "                        # Safely update each metric if it exists\n",
        "                        metrics_mapping = {\n",
        "                            'train_loss': 'loss',\n",
        "                            'val_loss': 'val_loss',\n",
        "                            'train_acc': 'accuracy',\n",
        "                            'val_acc': 'val_accuracy'\n",
        "                        }\n",
        "\n",
        "                        for history_key, fine_tune_key in metrics_mapping.items():\n",
        "                            if (history_key in self.metrics_history and\n",
        "                                fine_tune_key in fine_tune_history.history and\n",
        "                                isinstance(self.metrics_history[history_key], list)):\n",
        "                                self.metrics_history[history_key].extend(fine_tune_history.history[fine_tune_key])\n",
        "\n",
        "                print(\"Fine-tuning completed successfully\")\n",
        "            except Exception as fit_error:\n",
        "                print(f\"Error during fine-tuning fit process: {fit_error}\")\n",
        "                print(\"Fine-tuning failed, continuing with the base model\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fine-tuning preparation: {e}\")\n",
        "            print(\"Skipping fine-tuning and keeping the base model\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # Save the model with comprehensive error handling\n",
        "    print(\"Saving the final model...\")\n",
        "    try:\n",
        "        # Ensure output directory exists\n",
        "        if not os.path.exists(self.output_path):\n",
        "            os.makedirs(self.output_path, exist_ok=True)\n",
        "\n",
        "        # First try to save in TensorFlow SavedModel format (most reliable)\n",
        "        model_save_path = os.path.join(self.output_path, 'final_model')\n",
        "\n",
        "        # Create subdirectory if needed\n",
        "        if not os.path.exists(os.path.dirname(model_save_path)):\n",
        "            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
        "\n",
        "        # Save with error handling\n",
        "        try:\n",
        "            self.model.save(model_save_path, save_format='tf')\n",
        "            print(f\"Model saved successfully to {model_save_path}\")\n",
        "        except Exception as tf_save_error:\n",
        "            print(f\"Error saving in TensorFlow format: {tf_save_error}\")\n",
        "            # Continue to try other formats\n",
        "\n",
        "        # Try to save in H5 format\n",
        "        try:\n",
        "            h5_path = os.path.join(self.output_path, 'final_model.h5')\n",
        "            self.model.save(h5_path)\n",
        "            print(f\"Model saved in H5 format to {h5_path}\")\n",
        "        except Exception as h5_save_error:\n",
        "            print(f\"Warning: Could not save in H5 format: {h5_save_error}\")\n",
        "\n",
        "        # Save model architecture as JSON\n",
        "        try:\n",
        "            json_path = os.path.join(self.output_path, 'model_architecture.json')\n",
        "            with open(json_path, 'w') as f:\n",
        "                model_json = self.model.to_json()\n",
        "                f.write(model_json)\n",
        "            print(f\"Model architecture saved to {json_path}\")\n",
        "        except Exception as json_save_error:\n",
        "            print(f\"Warning: Could not save model architecture: {json_save_error}\")\n",
        "\n",
        "        # Save model weights as a separate file\n",
        "        try:\n",
        "            weights_path = os.path.join(self.output_path, 'model_weights.h5')\n",
        "            self.model.save_weights(weights_path)\n",
        "            print(f\"Model weights saved to {weights_path}\")\n",
        "        except Exception as weights_save_error:\n",
        "            print(f\"Warning: Could not save model weights: {weights_save_error}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error during model saving: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Last-ditch effort to save something\n",
        "        try:\n",
        "            print(\"Attempting emergency save of model weights...\")\n",
        "            emergency_path = os.path.join(self.output_path, 'emergency_weights.h5')\n",
        "\n",
        "            # Ensure the directory exists\n",
        "            os.makedirs(os.path.dirname(emergency_path), exist_ok=True)\n",
        "\n",
        "            # Try to save weights in HDF5 format\n",
        "            self.model.save_weights(emergency_path)\n",
        "            print(f\"Emergency save successful: {emergency_path}\")\n",
        "        except Exception as emergency_error:\n",
        "            print(f\"Emergency save failed. Model could not be saved: {emergency_error}\")\n",
        "            # Try one more approach with NumPy format\n",
        "            try:\n",
        "                np_path = os.path.join(self.output_path, 'emergency_weights.npz')\n",
        "                np_weights = [w.numpy() for w in self.model.weights]\n",
        "                np.savez(np_path, *np_weights)\n",
        "                print(f\"Emergency NumPy save successful: {np_path}\")\n",
        "            except:\n",
        "                print(\"All save attempts failed. Model could not be saved.\")\n",
        "\n",
        "    # Safely return history\n",
        "    if hasattr(history, 'history') and isinstance(history.history, dict):\n",
        "        return history.history\n",
        "    else:\n",
        "        print(\"Warning: No valid history found\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "ahPEXZ9t2c4M"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate the trained model with comprehensive metrics\n",
        "\n",
        "        Args:\n",
        "            X_test (numpy.ndarray): Test images\n",
        "            y_test (numpy.ndarray): Test labels\n",
        "\n",
        "        Returns:\n",
        "            dict: Evaluation metrics\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Please train the model first.\")\n",
        "\n",
        "        print(\"Evaluating model on test data...\")\n",
        "\n",
        "        try:\n",
        "            # Load label encoder\n",
        "            label_encoder_path = os.path.join(self.output_path, 'label_encoder.pkl')\n",
        "            if os.path.exists(label_encoder_path):\n",
        "                with open(label_encoder_path, 'rb') as f:\n",
        "                    label_encoder = pickle.load(f)\n",
        "                class_names = label_encoder.classes_\n",
        "            else:\n",
        "                # Default class names\n",
        "                class_names = ['inhibitory', 'excitatory']\n",
        "\n",
        "            # Make predictions in batches to avoid memory issues\n",
        "            batch_size = 32\n",
        "            n_batches = int(np.ceil(len(X_test) / batch_size))\n",
        "\n",
        "            y_pred_prob = []\n",
        "            for i in tqdm(range(n_batches), desc=\"Predicting\"):\n",
        "                start_idx = i * batch_size\n",
        "                end_idx = min((i + 1) * batch_size, len(X_test))\n",
        "                batch_pred = self.model.predict(X_test[start_idx:end_idx], verbose=0)\n",
        "                y_pred_prob.append(batch_pred)\n",
        "\n",
        "            y_pred_prob = np.vstack(y_pred_prob)\n",
        "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "            # Calculate comprehensive metrics\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            report = classification_report(y_test, y_pred, output_dict=True,\n",
        "                                          target_names=class_names)\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "            # Calculate per-class metrics\n",
        "            class_precision = {}\n",
        "            class_recall = {}\n",
        "            class_f1 = {}\n",
        "\n",
        "            for i, class_name in enumerate(class_names):\n",
        "                # True positives, false positives, true negatives, false negatives\n",
        "                tp = cm[i, i]\n",
        "                fp = cm[:, i].sum() - tp\n",
        "                fn = cm[i, :].sum() - tp\n",
        "                tn = cm.sum() - (tp + fp + fn)\n",
        "\n",
        "                # Precision, recall, F1\n",
        "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "                class_precision[class_name] = precision\n",
        "                class_recall[class_name] = recall\n",
        "                class_f1[class_name] = f1\n",
        "\n",
        "            # Create enhanced confusion matrix visualization\n",
        "            plt.figure(figsize=(12, 10))\n",
        "\n",
        "            # Normalize confusion matrix for better visualization\n",
        "            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "            # Create a more informative heatmap\n",
        "            ax = sns.heatmap(cm_norm, annot=cm, fmt='d', cmap='Blues',\n",
        "                           xticklabels=class_names,\n",
        "                           yticklabels=class_names)\n",
        "\n",
        "            # Add percentage text to each cell\n",
        "            for i in range(cm.shape[0]):\n",
        "                for j in range(cm.shape[1]):\n",
        "                    text = ax.texts[i * cm.shape[1] + j]\n",
        "                    percentage = cm_norm[i, j] * 100\n",
        "                    text.set_text(f\"{cm[i, j]}\\n({percentage:.1f}%)\")\n",
        "\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.title('Confusion Matrix with Class Distribution')\n",
        "\n",
        "            # Add colored borders based on correct/incorrect\n",
        "            for i in range(len(class_names)):\n",
        "                for j in range(len(class_names)):\n",
        "                    color = 'green' if i == j else 'red'\n",
        "                    plt.plot([j, j+1], [i, i], color=color, lw=2)\n",
        "                    plt.plot([j, j+1], [i+1, i+1], color=color, lw=2)\n",
        "                    plt.plot([j, j], [i, i+1], color=color, lw=2)\n",
        "                    plt.plot([j+1, j+1], [i, i+1], color=color, lw=2)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_path, 'confusion_matrix.png'), dpi=300)\n",
        "\n",
        "            # Create ROC curve for binary classification\n",
        "            if len(class_names) == 2:\n",
        "                from sklearn.metrics import roc_curve, auc\n",
        "                fpr, tpr, _ = roc_curve(y_test, y_pred_prob[:, 1])\n",
        "                roc_auc = auc(fpr, tpr)\n",
        "\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "                plt.xlim([0.0, 1.0])\n",
        "                plt.ylim([0.0, 1.05])\n",
        "                plt.xlabel('False Positive Rate')\n",
        "                plt.ylabel('True Positive Rate')\n",
        "                plt.title('Receiver Operating Characteristic')\n",
        "                plt.legend(loc=\"lower right\")\n",
        "                plt.savefig(os.path.join(self.output_path, 'roc_curve.png'), dpi=300)\n",
        "\n",
        "            # Create precision-recall curve\n",
        "            from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "\n",
        "            for i, class_name in enumerate(class_names):\n",
        "                if len(class_names) == 2 and i == 0:\n",
        "                    # For binary classification, only show the positive class\n",
        "                    continue\n",
        "\n",
        "                # Convert to one-vs-rest for multiclass\n",
        "                y_true_ovr = (y_test == i).astype(int)\n",
        "                y_score_ovr = y_pred_prob[:, i]\n",
        "\n",
        "                precision, recall, _ = precision_recall_curve(y_true_ovr, y_score_ovr)\n",
        "                avg_precision = average_precision_score(y_true_ovr, y_score_ovr)\n",
        "\n",
        "                plt.plot(recall, precision, lw=2,\n",
        "                         label=f'{class_name} (AP = {avg_precision:.2f})')\n",
        "\n",
        "            plt.xlabel('Recall')\n",
        "            plt.ylabel('Precision')\n",
        "            plt.title('Precision-Recall Curve')\n",
        "            plt.legend()\n",
        "            plt.savefig(os.path.join(self.output_path, 'precision_recall_curve.png'), dpi=300)\n",
        "\n",
        "            # Create a histogram of prediction confidences\n",
        "            plt.figure(figsize=(10, 6))\n",
        "\n",
        "            for i, class_name in enumerate(class_names):\n",
        "                # Get samples that are actually this class\n",
        "                class_indices = np.where(y_test == i)[0]\n",
        "                if len(class_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Get prediction confidences for this class\n",
        "                confidences = y_pred_prob[class_indices, i]\n",
        "\n",
        "                plt.hist(confidences, alpha=0.7, bins=20,\n",
        "                         label=f'True {class_name} samples', density=True)\n",
        "\n",
        "            plt.xlabel('Model Confidence')\n",
        "            plt.ylabel('Density')\n",
        "            plt.title('Distribution of Model Confidence for True Samples')\n",
        "            plt.legend()\n",
        "            plt.savefig(os.path.join(self.output_path, 'confidence_distribution.png'), dpi=300)\n",
        "\n",
        "            # Print detailed results\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Model Evaluation Results:\")\n",
        "            print(f\"{'='*50}\")\n",
        "            print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"\\nClassification Report:\")\n",
        "            print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "            print(f\"\\nConfusion Matrix:\")\n",
        "            print(cm)\n",
        "\n",
        "            print(f\"\\nPer-Class Metrics:\")\n",
        "            for class_name in class_names:\n",
        "                print(f\"  {class_name.capitalize()}:\")\n",
        "                print(f\"    Precision: {class_precision[class_name]:.4f}\")\n",
        "                print(f\"    Recall: {class_recall[class_name]:.4f}\")\n",
        "                print(f\"    F1-score: {class_f1[class_name]:.4f}\")\n",
        "\n",
        "            # Save detailed metrics to CSV\n",
        "            metrics_df = pd.DataFrame({\n",
        "                'Class': class_names,\n",
        "                'Precision': [class_precision[c] for c in class_names],\n",
        "                'Recall': [class_recall[c] for c in class_names],\n",
        "                'F1-Score': [class_f1[c] for c in class_names],\n",
        "                'Support': [report[c]['support'] for c in class_names]\n",
        "            })\n",
        "\n",
        "            metrics_df.to_csv(os.path.join(self.output_path, 'class_metrics.csv'), index=False)\n",
        "\n",
        "            # Return comprehensive metrics dictionary\n",
        "            return {\n",
        "                'accuracy': accuracy,\n",
        "                'classification_report': report,\n",
        "                'confusion_matrix': cm,\n",
        "                'class_precision': class_precision,\n",
        "                'class_recall': class_recall,\n",
        "                'class_f1': class_f1,\n",
        "                'pred_probabilities': y_pred_prob\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during model evaluation: {e}\")\n",
        "\n",
        "            # Attempt a simpler evaluation as fallback\n",
        "            try:\n",
        "                y_pred = np.argmax(self.model.predict(X_test), axis=1)\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "                print(f\"Fallback evaluation - Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "                # Basic confusion matrix\n",
        "                cm = confusion_matrix(y_test, y_pred)\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                sns.heatmap(cm, annot=True, fmt='d')\n",
        "                plt.xlabel('Predicted')\n",
        "                plt.ylabel('True')\n",
        "                plt.title('Confusion Matrix (Fallback)')\n",
        "                plt.savefig(os.path.join(self.output_path, 'confusion_matrix_fallback.png'))\n",
        "\n",
        "                return {\n",
        "                    'accuracy': accuracy,\n",
        "                    'confusion_matrix': cm\n",
        "                }\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"Fallback evaluation also failed: {e2}\")\n",
        "                return {\n",
        "                    'accuracy': None,\n",
        "                    'error': str(e)\n",
        "                }"
      ],
      "metadata": {
        "id": "yaS6QvjNS0TK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model_attention(self, image, true_label=None, layer_name=None, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize what the model focuses on using Grad-CAM with error handling\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Input image\n",
        "        true_label (int, optional): True label for the image\n",
        "        layer_name (str, optional): Name of the layer to use for Grad-CAM\n",
        "        save_path (str, optional): Path to save the visualization\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Heatmap overlay on the original image\n",
        "    \"\"\"\n",
        "    if self.model is None:\n",
        "        raise ValueError(\"Model has not been trained yet. Please train the model first.\")\n",
        "\n",
        "    try:\n",
        "        # Handle NaN or inf values in input\n",
        "        if np.isnan(image).any() or np.isinf(image).any():\n",
        "            print(\"Warning: Input image contains NaN or inf values. Fixing...\")\n",
        "            image = np.nan_to_num(image)\n",
        "\n",
        "        # Preprocess the image\n",
        "        img = self._preprocess_image(image)\n",
        "        img_array = np.expand_dims(img, axis=0)\n",
        "\n",
        "        # Get the model's prediction\n",
        "        preds = self.model.predict(img_array, verbose=0)\n",
        "        pred_class = np.argmax(preds[0])\n",
        "        pred_prob = preds[0][pred_class]\n",
        "\n",
        "        # Find appropriate layer for Grad-CAM\n",
        "        if layer_name is None:\n",
        "            # Try to find the last convolutional layer\n",
        "            found_conv_layer = False\n",
        "\n",
        "            # For models with a backbone (like EfficientNet), navigate to the backbone\n",
        "            if isinstance(self.model.layers[0], tf.keras.models.Model):\n",
        "                base_model = self.model.layers[0]\n",
        "                for layer in reversed(base_model.layers):\n",
        "                    if isinstance(layer, layers.Conv2D):\n",
        "                        layer_name = layer.name\n",
        "                        found_conv_layer = True\n",
        "                        break\n",
        "\n",
        "            # If not found in backbone or no backbone exists, check the main model\n",
        "            if not found_conv_layer:\n",
        "                for layer in reversed(self.model.layers):\n",
        "                    if isinstance(layer, layers.Conv2D):\n",
        "                        layer_name = layer.name\n",
        "                        found_conv_layer = True\n",
        "                        break\n",
        "\n",
        "            # If still no conv layer, try to find any layer with a 4D output (NHWC format)\n",
        "            if not found_conv_layer:\n",
        "                for layer in reversed(self.model.layers):\n",
        "                    if len(getattr(layer, 'output_shape', [])) == 4:\n",
        "                        layer_name = layer.name\n",
        "                        found_conv_layer = True\n",
        "                        break\n",
        "\n",
        "            if not found_conv_layer:\n",
        "                raise ValueError(\"Could not find appropriate layer for Grad-CAM visualization\")\n",
        "\n",
        "        # Support for different model architectures\n",
        "        try:\n",
        "            if isinstance(self.model.layers[0], tf.keras.models.Model):\n",
        "                # Get the layer from the base model\n",
        "                target_layer = self.model.layers[0].get_layer(layer_name)\n",
        "                grad_model = models.Model(\n",
        "                    inputs=[self.model.inputs],\n",
        "                    outputs=[target_layer.output, self.model.output]\n",
        "                )\n",
        "            else:\n",
        "                # Standard model\n",
        "                grad_model = models.Model(\n",
        "                    inputs=[self.model.inputs],\n",
        "                    outputs=[self.model.get_layer(layer_name).output, self.model.output]\n",
        "                )\n",
        "        except:\n",
        "            # Fallback approach if we can't create a proper grad model\n",
        "            print(\"Warning: Using fallback approach for Grad-CAM\")\n",
        "            # Create a simpler heatmap based on the model's attention\n",
        "            superimposed_img = self._create_attention_heatmap(image, img_array, pred_class)\n",
        "\n",
        "            # Create a figure with the fallback result\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(np.array(image * 255, dtype=np.uint8))\n",
        "            plt.title('Original Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(superimposed_img)\n",
        "            title = f'Attention Map - {[\"Inhibitory\", \"Excitatory\"][pred_class]} ({pred_prob:.2f})'\n",
        "            plt.title(title)\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            if save_path:\n",
        "                plt.savefig(save_path, dpi=300)\n",
        "            else:\n",
        "                plt.savefig(os.path.join(self.output_path, 'fallback_attention_map.png'))\n",
        "\n",
        "            plt.close()  # Close the figure to free memory\n",
        "            return superimposed_img\n",
        "\n",
        "        # Compute gradients with error handling\n",
        "        try:\n",
        "            with tf.GradientTape() as tape:\n",
        "                conv_output, predictions = grad_model(img_array)\n",
        "                loss = predictions[:, pred_class]\n",
        "\n",
        "            # Gradients of the top predicted class with respect to the output feature map\n",
        "            grads = tape.gradient(loss, conv_output)\n",
        "\n",
        "            # Handle potential null gradients\n",
        "            if grads is None:\n",
        "                raise ValueError(\"Gradient is None - check model architecture\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing gradients: {e}\")\n",
        "            # Fallback to simple attention visualization\n",
        "            superimposed_img = self._create_attention_heatmap(image, img_array, pred_class)\n",
        "            return superimposed_img\n",
        "\n",
        "        # Vector where each entry is the mean intensity of the gradient over a channel\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "        # Weight the channels by the mean gradient\n",
        "        conv_output = conv_output[0]\n",
        "        for i in range(pooled_grads.shape[-1]):\n",
        "            conv_output[:, :, i] *= pooled_grads[i]\n",
        "\n",
        "        # Average over all the feature maps\n",
        "        heatmap = tf.reduce_mean(conv_output, axis=-1)\n",
        "\n",
        "        # Normalize the heatmap\n",
        "        heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + tf.keras.backend.epsilon())\n",
        "        heatmap = heatmap.numpy()\n",
        "\n",
        "        # Resize the heatmap to the original image size\n",
        "        heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "\n",
        "        # Convert to RGB if the image is grayscale\n",
        "        if len(image.shape) == 2 or (len(image.shape) == 3 and image.shape[2] == 1):\n",
        "            if len(image.shape) == 3:\n",
        "                image = image[:, :, 0]\n",
        "            image_rgb = np.stack([image] * 3, axis=-1)\n",
        "        else:\n",
        "            image_rgb = image\n",
        "\n",
        "        # Ensure image is in correct range\n",
        "        if image_rgb.max() <= 1.0:\n",
        "            image_rgb = (image_rgb * 255).astype(np.uint8)\n",
        "\n",
        "        # Apply colormap to the heatmap\n",
        "        heatmap = np.uint8(255 * heatmap)\n",
        "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "        # Superimpose the heatmap on the image\n",
        "        superimposed_img = (heatmap * 0.4 + image_rgb * 0.6).astype(np.uint8)\n",
        "\n",
        "        # Create a figure\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Original image\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(image_rgb)\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Heatmap\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(heatmap)\n",
        "        plt.title('Attention Heatmap')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Superimposed image\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(superimposed_img)\n",
        "\n",
        "        # Add class label and confidence score to title\n",
        "        title = f'Predicted: {[\"Inhibitory\", \"Excitatory\"][pred_class]} ({pred_prob:.2f})'\n",
        "        if true_label is not None:\n",
        "            title += f', True: {[\"Inhibitory\", \"Excitatory\"][true_label]}'\n",
        "        plt.title(title)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300)\n",
        "        else:\n",
        "            plt.savefig(os.path.join(self.output_path, 'grad_cam_visualization.png'), dpi=300)\n",
        "\n",
        "        plt.close()  # Close the figure to free memory\n",
        "        return superimposed_img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Grad-CAM visualization: {e}\")\n",
        "        # Create a simple fallback visualization\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        try:\n",
        "            # Convert image to displayable format\n",
        "            display_img = np.copy(image)\n",
        "            if display_img.max() <= 1.0:\n",
        "                display_img = (display_img * 255).astype(np.uint8)\n",
        "\n",
        "            plt.imshow(display_img)\n",
        "            plt.title(\"Visualization failed - Showing original image\")\n",
        "            plt.axis('off')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save fallback visualization\n",
        "            fallback_path = os.path.join(self.output_path, 'fallback_visualization.png')\n",
        "            plt.savefig(fallback_path)\n",
        "            print(f\"Fallback image saved to {fallback_path}\")\n",
        "\n",
        "            # Close the figure to free memory\n",
        "            plt.close()\n",
        "\n",
        "            # Return the normalized image for consistent output\n",
        "            if image.max() <= 1.0:\n",
        "                return (image * 255).astype(np.uint8)\n",
        "            else:\n",
        "                return image.astype(np.uint8)\n",
        "        except Exception as disp_error:\n",
        "            print(f\"Could not display fallback image: {disp_error}\")\n",
        "            # Create an empty image of the same shape as input\n",
        "            if hasattr(image, 'shape'):\n",
        "                empty_img = np.zeros_like(image)\n",
        "                if len(empty_img.shape) == 2:\n",
        "                    empty_img = np.stack([empty_img] * 3, axis=-1)\n",
        "                return empty_img.astype(np.uint8)\n",
        "            else:\n",
        "                # Last resort - create a small blank image\n",
        "                return np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "\n",
        "def _create_attention_heatmap(self, image, preprocessed_input, pred_class):\n",
        "    \"\"\"\n",
        "    Create a simple attention heatmap as fallback for Grad-CAM\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Original image\n",
        "        preprocessed_input (numpy.ndarray): Preprocessed input batch\n",
        "        pred_class (int): Predicted class index\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Simple attention heatmap\n",
        "    \"\"\"\n",
        "    # Create a simplified attention map when Grad-CAM fails\n",
        "    # Use blur and edge detection as a proxy for attention\n",
        "\n",
        "    try:\n",
        "        # Convert to grayscale if needed\n",
        "        if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "            gray = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = (image * 255).astype(np.uint8)\n",
        "\n",
        "        # Apply Gaussian blur\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "        # Apply edge detection (Canny or Sobel)\n",
        "        edges = cv2.Canny(blurred, 50, 150)\n",
        "\n",
        "        # Combine edge detection with blurring for an attention-like effect\n",
        "        attention_map = cv2.GaussianBlur(edges, (15, 15), 0)\n",
        "\n",
        "        # Normalize to [0, 1]\n",
        "        attention_map = attention_map / (attention_map.max() + 1e-8)  # Avoid division by zero\n",
        "\n",
        "        # Apply color map\n",
        "        heatmap = cv2.applyColorMap((attention_map * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "\n",
        "        # Create RGB version of original image\n",
        "        if len(image.shape) == 2 or (len(image.shape) == 3 and image.shape[2] == 1):\n",
        "            if len(image.shape) == 3:\n",
        "                image = image[:, :, 0]\n",
        "            rgb_img = np.stack([image] * 3, axis=-1)\n",
        "        else:\n",
        "            rgb_img = image\n",
        "\n",
        "        if rgb_img.max() <= 1.0:\n",
        "            rgb_img = (rgb_img * 255).astype(np.uint8)\n",
        "\n",
        "        # Overlay heatmap on image\n",
        "        superimposed = cv2.addWeighted(heatmap, 0.4, rgb_img, 0.6, 0)\n",
        "\n",
        "        return superimposed\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating fallback attention map: {e}\")\n",
        "        # If all else fails, return the original image\n",
        "        return (image * 255).astype(np.uint8) if image.max() <= 1.0 else image.astype(np.uint8)"
      ],
      "metadata": {
        "id": "CXXg9E9M3Quo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_synapse_type(self, image, threshold=0.5, return_visualization=False):\n",
        "        \"\"\"\n",
        "        Predict whether a synapse is excitatory or inhibitory with confidence threshold\n",
        "\n",
        "        Args:\n",
        "            image (numpy.ndarray): Input EM image of a synapse\n",
        "            threshold (float): Confidence threshold for prediction\n",
        "            return_visualization (bool): Whether to return a visualization of the prediction\n",
        "\n",
        "        Returns:\n",
        "            tuple: (prediction class, prediction probability, visualization if requested)\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Please train the model first.\")\n",
        "\n",
        "        try:\n",
        "            # Preprocess the image\n",
        "            img = self._preprocess_image(image)\n",
        "            img_array = np.expand_dims(img, axis=0)\n",
        "\n",
        "            # Get the model's prediction\n",
        "            preds = self.model.predict(img_array, verbose=0)\n",
        "            pred_class = np.argmax(preds[0])\n",
        "            pred_prob = preds[0][pred_class]\n",
        "\n",
        "            # Get confidence level\n",
        "            confidence = pred_prob\n",
        "\n",
        "            # Load the label encoder\n",
        "            try:\n",
        "                with open(os.path.join(self.output_path, 'label_encoder.pkl'), 'rb') as f:\n",
        "                    label_encoder = pickle.load(f)\n",
        "\n",
        "                # Convert predicted class to label\n",
        "                pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
        "            except:\n",
        "                # Fallback if label encoder not available\n",
        "                pred_label = \"excitatory\" if pred_class == 1 else \"inhibitory\"\n",
        "\n",
        "            # Apply confidence threshold\n",
        "            if confidence < threshold:\n",
        "                pred_label = \"uncertain\"\n",
        "\n",
        "            # Create visualization if requested\n",
        "            if return_visualization:\n",
        "                # Create a visualization of the prediction\n",
        "                viz_path = os.path.join(self.output_path, f'prediction_{np.random.randint(10000)}.png')\n",
        "                viz_img = self.visualize_model_attention(image, layer_name=None, save_path=viz_path)\n",
        "                return pred_label, pred_prob, viz_img\n",
        "\n",
        "            return pred_label, pred_prob\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting synapse type: {e}\")\n",
        "            if return_visualization:\n",
        "                return \"error\", 0.0, None\n",
        "            return \"error\", 0.0"
      ],
      "metadata": {
        "id": "mrBz0HyMTN4a"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_synapse_distribution(self, volume_path, segmentation_path, output_file, confidence_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Map the distribution of excitatory and inhibitory synapses in a volume with optimized memory handling\n",
        "\n",
        "    Args:\n",
        "        volume_path (str): Path to the EM volume\n",
        "        segmentation_path (str): Path to the synapse segmentation\n",
        "        output_file (str): Path to save the distribution map\n",
        "        confidence_threshold (float): Threshold for prediction confidence\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing synapse counts and distribution\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the volume and segmentation with memory efficient approach\n",
        "        print(f\"Loading volume from {volume_path}...\")\n",
        "\n",
        "        # Check file size before loading to handle large volumes\n",
        "        volume_size = os.path.getsize(volume_path) / (1024 * 1024 * 1024)  # Size in GB\n",
        "        print(f\"Volume file size: {volume_size:.2f} GB\")\n",
        "\n",
        "        # Use chunked loading for large volumes\n",
        "        if volume_size > 2.0:  # More than 2GB\n",
        "            print(\"Large volume detected, using chunked loading...\")\n",
        "\n",
        "            with h5py.File(volume_path, 'r') as f:\n",
        "                # Get volume dimensions and key\n",
        "                if 'em_volume' in f:\n",
        "                    volume_key = 'em_volume'\n",
        "                else:\n",
        "                    # Find the first dataset in the file\n",
        "                    for key in f.keys():\n",
        "                        if isinstance(f[key], h5py.Dataset):\n",
        "                            volume_key = key\n",
        "                            break\n",
        "                    else:\n",
        "                        raise ValueError(\"Could not find a dataset in the volume file\")\n",
        "\n",
        "                volume_shape = f[volume_key].shape\n",
        "                print(f\"Volume shape: {volume_shape}\")\n",
        "\n",
        "                # Load segmentation\n",
        "                print(f\"Loading segmentation from {segmentation_path}...\")\n",
        "                with h5py.File(segmentation_path, 'r') as seg_f:\n",
        "                    # Find segmentation dataset\n",
        "                    if 'synapse_mask' in seg_f:\n",
        "                        seg_key = 'synapse_mask'\n",
        "                    else:\n",
        "                        # Find the first dataset\n",
        "                        for key in seg_f.keys():\n",
        "                            if isinstance(seg_f[key], h5py.Dataset):\n",
        "                                seg_key = key\n",
        "                                break\n",
        "                        else:\n",
        "                            raise ValueError(\"Could not find a dataset in the segmentation file\")\n",
        "\n",
        "                    # Load entire segmentation if it's not too large\n",
        "                    seg_size = os.path.getsize(segmentation_path) / (1024 * 1024 * 1024)\n",
        "                    if seg_size < 1.0:  # Less than 1GB\n",
        "                        segmentation = seg_f[seg_key][:]\n",
        "                    else:\n",
        "                        # Process segmentation in chunks\n",
        "                        print(\"Large segmentation detected, finding synapse coordinates first...\")\n",
        "                        # Find connected components in chunks\n",
        "                        synapse_coords = []\n",
        "\n",
        "                        # Process in slabs along z-axis\n",
        "                        chunk_size = min(50, volume_shape[0] // 10)  # Process ~10 chunks or fewer\n",
        "                        for z_start in range(0, volume_shape[0], chunk_size):\n",
        "                            z_end = min(z_start + chunk_size, volume_shape[0])\n",
        "                            print(f\"Processing segmentation chunk {z_start}-{z_end}...\")\n",
        "\n",
        "                            # Load chunk of segmentation\n",
        "                            seg_chunk = seg_f[seg_key][z_start:z_end, :, :]\n",
        "\n",
        "                            # Find synapses in this chunk\n",
        "                            labeled_chunk, num_features = ndi.label(seg_chunk)\n",
        "\n",
        "                            # For each synapse in this chunk\n",
        "                            for i in range(1, num_features + 1):\n",
        "                                # Get coordinates\n",
        "                                coords = np.where(labeled_chunk == i)\n",
        "                                if len(coords[0]) == 0:\n",
        "                                    continue\n",
        "\n",
        "                                # Adjust z coordinates for chunk offset\n",
        "                                mean_coords = [\n",
        "                                    coords[0].mean() + z_start,\n",
        "                                    coords[1].mean(),\n",
        "                                    coords[2].mean()\n",
        "                                ]\n",
        "\n",
        "                                synapse_coords.append(mean_coords)\n",
        "\n",
        "                        # Create placeholder for synapse regions\n",
        "                        patches = []\n",
        "\n",
        "                        # Extract each synapse from the volume\n",
        "                        patch_size = 64  # Fixed size for each synapse patch\n",
        "                        for coords in tqdm(synapse_coords, desc=\"Extracting synapse patches\"):\n",
        "                            z, y, x = [int(c) for c in coords]\n",
        "\n",
        "                            # Create bounds with padding\n",
        "                            z_min = max(0, z - patch_size // 2)\n",
        "                            z_max = min(volume_shape[0], z + patch_size // 2)\n",
        "                            y_min = max(0, y - patch_size // 2)\n",
        "                            y_max = min(volume_shape[1], y + patch_size // 2)\n",
        "                            x_min = max(0, x - patch_size // 2)\n",
        "                            x_max = min(volume_shape[2], x + patch_size // 2)\n",
        "\n",
        "                            # Extract patch from volume\n",
        "                            with h5py.File(volume_path, 'r') as vol_f:\n",
        "                                patch = vol_f[volume_key][z_min:z_max, y_min:y_max, x_min:x_max]\n",
        "\n",
        "                                # Take middle slice or max projection for 2D representation\n",
        "                                if patch.shape[0] > 0:\n",
        "                                    middle_slice = patch[patch.shape[0] // 2]\n",
        "                                    patches.append(middle_slice)\n",
        "\n",
        "                        # Skip the regular extraction process\n",
        "                        print(f\"Extracted {len(patches)} synapse patches directly\")\n",
        "                        # Predict types directly\n",
        "                        synapse_types = []\n",
        "                        confidences = []\n",
        "\n",
        "                        for patch in tqdm(patches, desc=\"Predicting synapse types\"):\n",
        "                            syn_type, conf = self.predict_synapse_type(patch, threshold=confidence_threshold)\n",
        "                            synapse_types.append(syn_type)\n",
        "                            confidences.append(conf)\n",
        "\n",
        "                        # Skip to visualization\n",
        "                        goto_visualization = True\n",
        "        else:\n",
        "            # Standard loading for smaller volumes\n",
        "            with h5py.File(volume_path, 'r') as f:\n",
        "                # Find the volume dataset\n",
        "                if 'em_volume' in f:\n",
        "                    volume = f['em_volume'][:]\n",
        "                else:\n",
        "                    # Find the first dataset\n",
        "                    for key in f.keys():\n",
        "                        if isinstance(f[key], h5py.Dataset):\n",
        "                            volume = f[key][:]\n",
        "                            break\n",
        "                    else:\n",
        "                        raise ValueError(\"Could not find a dataset in the volume file\")\n",
        "\n",
        "            print(f\"Loading segmentation from {segmentation_path}...\")\n",
        "            with h5py.File(segmentation_path, 'r') as f:\n",
        "                if 'synapse_mask' in f:\n",
        "                    segmentation = f['synapse_mask'][:]\n",
        "                else:\n",
        "                    # Find the first dataset\n",
        "                    for key in f.keys():\n",
        "                        if isinstance(f[key], h5py.Dataset):\n",
        "                            segmentation = f[key][:]\n",
        "                            break\n",
        "                    else:\n",
        "                        raise ValueError(\"Could not find a dataset in the segmentation file\")\n",
        "\n",
        "            # Standard extraction process\n",
        "            patches = self.extract_synapse_regions(volume, segmentation)\n",
        "            print(f\"Extracted {len(patches)} synapse patches\")\n",
        "\n",
        "            # Find connected components in the segmentation\n",
        "            labeled_mask, num_features = ndi.label(segmentation)\n",
        "\n",
        "            # Predict synapse types\n",
        "            synapse_types = []\n",
        "            confidences = []\n",
        "            synapse_coords = []\n",
        "\n",
        "            for i in tqdm(range(1, num_features + 1), desc=\"Predicting synapse types\"):\n",
        "                # Get coordinates for this synapse\n",
        "                coords = np.mean(np.where(labeled_mask == i), axis=1)\n",
        "                synapse_coords.append(coords)\n",
        "\n",
        "                # Get the patch for this synapse\n",
        "                patch_idx = i - 1  # 0-based indexing for patches\n",
        "                if patch_idx < len(patches):\n",
        "                    patch = patches[patch_idx]\n",
        "\n",
        "                    # Predict the type\n",
        "                    syn_type, conf = self.predict_synapse_type(patch, threshold=confidence_threshold)\n",
        "                    synapse_types.append(syn_type)\n",
        "                    confidences.append(conf)\n",
        "                else:\n",
        "                    # Skip this synapse if patch not available\n",
        "                    synapse_types.append(\"unknown\")\n",
        "                    confidences.append(0.0)\n",
        "\n",
        "            goto_visualization = False\n",
        "\n",
        "        # Visualization and analysis\n",
        "        if 'goto_visualization' not in locals() or not goto_visualization:\n",
        "            # Filter by confidence if not already done\n",
        "            confident_indices = [i for i, conf in enumerate(confidences) if conf >= confidence_threshold]\n",
        "            filtered_types = [synapse_types[i] for i in confident_indices]\n",
        "            filtered_coords = [synapse_coords[i] for i in confident_indices]\n",
        "            filtered_confidences = [confidences[i] for i in confident_indices]\n",
        "\n",
        "            print(f\"Using {len(filtered_types)} synapses with confidence >= {confidence_threshold}\")\n",
        "        else:\n",
        "            # These were already filtered by confidence threshold in the prediction step\n",
        "            filtered_types = synapse_types\n",
        "            filtered_coords = synapse_coords\n",
        "            filtered_confidences = confidences\n",
        "\n",
        "        # Count the number of each type\n",
        "        synapse_counts = pd.Series(filtered_types).value_counts()\n",
        "        print(\"\\nSynapse Type Distribution:\")\n",
        "        for syn_type, count in synapse_counts.items():\n",
        "            percentage = (count / len(filtered_types)) * 100\n",
        "            print(f\"  {syn_type.capitalize()}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "        # Create enhanced visualizations\n",
        "\n",
        "        # 1. 3D interactive visualization with plotly if available\n",
        "        try:\n",
        "            import plotly.graph_objects as go\n",
        "            from plotly.subplots import make_subplots\n",
        "\n",
        "            # Create interactive 3D plot\n",
        "            fig = make_subplots(\n",
        "                rows=1, cols=2,\n",
        "                specs=[[{'type': 'scatter3d'}, {'type': 'pie'}]],\n",
        "                subplot_titles=('Synapse Distribution in 3D', 'Synapse Type Proportions')\n",
        "            )\n",
        "\n",
        "            # Define colors for each type\n",
        "            color_map = {\n",
        "                'excitatory': 'red',\n",
        "                'inhibitory': 'blue',\n",
        "                'uncertain': 'gray',\n",
        "                'unknown': 'black'\n",
        "            }\n",
        "\n",
        "            # Create separate traces for each type for better interactive legend\n",
        "            for syn_type in set(filtered_types):\n",
        "                # Get indices for this type\n",
        "                type_indices = [i for i, t in enumerate(filtered_types) if t == syn_type]\n",
        "\n",
        "                # Extract coordinates and confidences for this type\n",
        "                type_coords = [filtered_coords[i] for i in type_indices]\n",
        "                type_confs = [filtered_confidences[i] for i in type_indices]\n",
        "\n",
        "                if not type_coords:\n",
        "                    continue\n",
        "\n",
        "                # Create 3D scatter plot for this type\n",
        "                x = [coord[2] for coord in type_coords]\n",
        "                y = [coord[1] for coord in type_coords]\n",
        "                z = [coord[0] for coord in type_coords]\n",
        "\n",
        "                # Size points by confidence\n",
        "                marker_size = [conf * 10 + 5 for conf in type_confs]\n",
        "\n",
        "                fig.add_trace(\n",
        "                    go.Scatter3d(\n",
        "                        x=x, y=y, z=z,\n",
        "                        mode='markers',\n",
        "                        marker=dict(\n",
        "                            size=marker_size,\n",
        "                            color=color_map.get(syn_type, 'purple'),\n",
        "                            opacity=0.8,\n",
        "                            symbol='circle'\n",
        "                        ),\n",
        "                        name=f\"{syn_type.capitalize()} Synapses\",\n",
        "                        hovertext=[f\"Type: {syn_type}<br>Confidence: {conf:.2f}\" for conf in type_confs]\n",
        "                    ),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "            # Add pie chart of proportions\n",
        "            labels = synapse_counts.index.tolist()\n",
        "            values = synapse_counts.values.tolist()\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Pie(\n",
        "                    labels=labels,\n",
        "                    values=values,\n",
        "                    textinfo='label+percent',\n",
        "                    marker=dict(\n",
        "                        colors=[color_map.get(label, 'purple') for label in labels]\n",
        "                    )\n",
        "                ),\n",
        "                row=1, col=2\n",
        "            )\n",
        "\n",
        "            # Update layout\n",
        "            fig.update_layout(\n",
        "                title_text=\"Synapse Distribution Analysis\",\n",
        "                height=800,\n",
        "                width=1200,\n",
        "                scene=dict(\n",
        "                    xaxis_title='X',\n",
        "                    yaxis_title='Y',\n",
        "                    zaxis_title='Z'\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Save as HTML for interactive exploration\n",
        "            html_output = os.path.splitext(output_file)[0] + '.html'\n",
        "            fig.write_html(html_output)\n",
        "            print(f\"Interactive 3D visualization saved to {html_output}\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Plotly not available, falling back to Matplotlib for visualization\")\n",
        "\n",
        "        # 2. Create a 3D visualization with matplotlib\n",
        "        fig = plt.figure(figsize=(12, 10))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        # Plot each synapse with varying size based on confidence\n",
        "        for i, (coords, syn_type, conf) in enumerate(zip(filtered_coords, filtered_types, filtered_confidences)):\n",
        "            # Determine color based on synapse type\n",
        "            if syn_type == 'excitatory':\n",
        "                color = 'red'\n",
        "            elif syn_type == 'inhibitory':\n",
        "                color = 'blue'\n",
        "            elif syn_type == 'uncertain':\n",
        "                color = 'gray'\n",
        "            else:\n",
        "                color = 'black'\n",
        "\n",
        "            # Size based on confidence\n",
        "            size = conf * 50 + 20\n",
        "\n",
        "            # Plot with alpha for better visualization of overlapping points\n",
        "            ax.scatter(\n",
        "                coords[2], coords[1], coords[0],\n",
        "                c=color,\n",
        "                s=size,\n",
        "                alpha=0.7,\n",
        "                edgecolors='white',\n",
        "                linewidth=0.5\n",
        "            )\n",
        "\n",
        "        ax.set_xlabel('X')\n",
        "        ax.set_ylabel('Y')\n",
        "        ax.set_zlabel('Z')\n",
        "\n",
        "        # Better title with statistics\n",
        "        ax.set_title(f'Distribution of Synapses\\n'\n",
        "                    f'Excitatory: {synapse_counts.get(\"excitatory\", 0)} | '\n",
        "                    f'Inhibitory: {synapse_counts.get(\"inhibitory\", 0)} | '\n",
        "                    f'Uncertain: {synapse_counts.get(\"uncertain\", 0)}')\n",
        "\n",
        "        # Add a legend\n",
        "        from matplotlib.lines import Line2D\n",
        "\n",
        "        legend_elements = []\n",
        "        for syn_type, color in [\n",
        "            ('Excitatory', 'red'),\n",
        "            ('Inhibitory', 'blue'),\n",
        "            ('Uncertain', 'gray'),\n",
        "            ('Unknown', 'black')\n",
        "        ]:\n",
        "            if syn_type.lower() in synapse_counts:\n",
        "                legend_elements.append(\n",
        "                    Line2D([0], [0], marker='o', color='w',\n",
        "                           label=f'{syn_type} ({synapse_counts.get(syn_type.lower(), 0)})',\n",
        "                           markerfacecolor=color, markersize=10)\n",
        "                )\n",
        "\n",
        "        ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1))\n",
        "\n",
        "        # Set axis limits with padding\n",
        "        if filtered_coords:\n",
        "            x_coords = [coord[2] for coord in filtered_coords]\n",
        "            y_coords = [coord[1] for coord in filtered_coords]\n",
        "            z_coords = [coord[0] for coord in filtered_coords]\n",
        "\n",
        "            padding = 20  # Add padding around min/max\n",
        "            ax.set_xlim(min(x_coords) - padding, max(x_coords) + padding)\n",
        "            ax.set_ylim(min(y_coords) - padding, max(y_coords) + padding)\n",
        "            ax.set_zlim(min(z_coords) - padding, max(z_coords) + padding)\n",
        "\n",
        "        # Improve grid and ticks for better visibility\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_file, dpi=300)\n",
        "        plt.close()  # Close figure to free memory\n",
        "        print(f\"3D visualization saved to {output_file}\")\n",
        "\n",
        "        # 3. Create a 2D density map for each axis plane\n",
        "        plt.figure(figsize=(18, 6))\n",
        "\n",
        "        # XY Plane (view from top)\n",
        "        plt.subplot(1, 3, 1)\n",
        "        for syn_type in ['excitatory', 'inhibitory']:\n",
        "            type_indices = [i for i, t in enumerate(filtered_types) if t == syn_type]\n",
        "            if not type_indices:\n",
        "                continue\n",
        "\n",
        "            type_coords = [filtered_coords[i] for i in type_indices]\n",
        "            xs = [coord[2] for coord in type_coords]\n",
        "            ys = [coord[1] for coord in type_coords]\n",
        "\n",
        "            color = 'red' if syn_type == 'excitatory' else 'blue'\n",
        "            plt.scatter(xs, ys, c=color, alpha=0.5, label=syn_type.capitalize())\n",
        "\n",
        "        plt.xlabel('X')\n",
        "        plt.ylabel('Y')\n",
        "        plt.title('Synapse Distribution (XY Plane)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # XZ Plane\n",
        "        plt.subplot(1, 3, 2)\n",
        "        for syn_type in ['excitatory', 'inhibitory']:\n",
        "            type_indices = [i for i, t in enumerate(filtered_types) if t == syn_type]\n",
        "            if not type_indices:\n",
        "                continue\n",
        "\n",
        "            type_coords = [filtered_coords[i] for i in type_indices]\n",
        "            xs = [coord[2] for coord in type_coords]\n",
        "            zs = [coord[0] for coord in type_coords]\n",
        "\n",
        "            color = 'red' if syn_type == 'excitatory' else 'blue'\n",
        "            plt.scatter(xs, zs, c=color, alpha=0.5, label=syn_type.capitalize())\n",
        "\n",
        "        plt.xlabel('X')\n",
        "        plt.ylabel('Z')\n",
        "        plt.title('Synapse Distribution (XZ Plane)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # YZ Plane\n",
        "        plt.subplot(1, 3, 3)\n",
        "        for syn_type in ['excitatory', 'inhibitory']:\n",
        "            type_indices = [i for i, t in enumerate(filtered_types) if t == syn_type]\n",
        "            if not type_indices:\n",
        "                continue\n",
        "\n",
        "            type_coords = [filtered_coords[i] for i in type_indices]\n",
        "            ys = [coord[1] for coord in type_coords]\n",
        "            zs = [coord[0] for coord in type_coords]\n",
        "\n",
        "            color = 'red' if syn_type == 'excitatory' else 'blue'\n",
        "            plt.scatter(ys, zs, c=color, alpha=0.5, label=syn_type.capitalize())\n",
        "\n",
        "        plt.xlabel('Y')\n",
        "        plt.ylabel('Z')\n",
        "        plt.title('Synapse Distribution (YZ Plane)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        planes_output = os.path.splitext(output_file)[0] + '_planes.png'\n",
        "        plt.savefig(planes_output, dpi=300)\n",
        "        plt.close()  # Close figure to free memory\n",
        "        print(f\"2D plane visualizations saved to {planes_output}\")\n",
        "\n",
        "        # Create distribution statistics dictionary\n",
        "        distribution = {\n",
        "            'synapse_counts': synapse_counts.to_dict(),\n",
        "            'synapse_coords': filtered_coords,\n",
        "            'synapse_types': filtered_types,\n",
        "            'synapse_confidences': filtered_confidences,\n",
        "            'e_i_ratio': synapse_counts.get('excitatory', 0) / max(1, synapse_counts.get('inhibitory', 0)),\n",
        "            'total_confident_synapses': len(filtered_types),\n",
        "            'visualization_files': [output_file, planes_output]\n",
        "        }\n",
        "\n",
        "        # Save the distribution data as JSON for future reference\n",
        "        import json\n",
        "\n",
        "        # Convert numpy arrays to lists for JSON serialization\n",
        "        json_safe_distribution = {\n",
        "            'synapse_counts': distribution['synapse_counts'],\n",
        "            'e_i_ratio': float(distribution['e_i_ratio']),\n",
        "            'total_confident_synapses': distribution['total_confident_synapses'],\n",
        "            'visualization_files': distribution['visualization_files'],\n",
        "            # Don't save coordinates/types in JSON (too large)\n",
        "        }\n",
        "\n",
        "        json_output = os.path.splitext(output_file)[0] + '_stats.json'\n",
        "        with open(json_output, 'w') as f:\n",
        "            json.dump(json_safe_distribution, f, indent=2)\n",
        "\n",
        "        print(f\"Distribution statistics saved to {json_output}\")\n",
        "\n",
        "        return distribution\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error mapping synapse distribution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Return basic error info\n",
        "        return {\n",
        "            'error': str(e),\n",
        "            'synapse_counts': {},\n",
        "            'total_synapses': 0\n",
        "        }"
      ],
      "metadata": {
        "id": "dXm14zOu3iuF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_synapse_distribution(self, volume_path, segmentation_path, output_file, confidence_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Map the distribution of excitatory and inhibitory synapses in a volume with optimized memory handling\n",
        "\n",
        "    Args:\n",
        "        volume_path (str): Path to the EM volume\n",
        "        segmentation_path (str): Path to the synapse segmentation\n",
        "        output_file (str): Path to save the distribution map\n",
        "        confidence_threshold (float): Threshold for prediction confidence\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing synapse counts and distribution\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the volume and segmentation with memory efficient approach\n",
        "        print(f\"Loading volume from {volume_path}...\")\n",
        "\n",
        "        # Check file size before loading to handle large volumes\n",
        "        volume_size = os.path.getsize(volume_path) / (1024 * 1024 * 1024)  # Size in GB\n",
        "        print(f\"Volume file size: {volume_size:.2f} GB\")\n",
        "\n",
        "        # Use chunked loading for large volumes\n",
        "        if volume_size > 2.0:  # More than 2GB\n",
        "            print(\"Large volume detected, using chunked loading...\")\n",
        "\n",
        "            with h5py.File(volume_path, 'r') as f:\n",
        "                # Get volume dimensions and key\n",
        "                if 'em_volume' in f:\n",
        "                    volume_key = 'em_volume'\n",
        "                else:\n",
        "                    # Find the first dataset in the file\n",
        "                    for key in f.keys():\n",
        "                        if isinstance(f[key], h5py.Dataset):\n",
        "                            volume_key = key\n",
        "                            break\n",
        "                    else:\n",
        "                        raise ValueError(\"Could not find a dataset in the volume file\")\n",
        "\n",
        "                volume_shape = f[volume_key].shape\n",
        "                print(f\"Volume shape: {volume_shape}\")\n",
        "\n",
        "                # Load segmentation\n",
        "                print(f\"Loading segmentation from {segmentation_path}...\")\n",
        "                with h5py.File(segmentation_path, 'r') as seg_f:\n",
        "                    # Find segmentation dataset\n",
        "                    if 'synapse_mask' in seg_f:\n",
        "                        seg_key = 'synapse_mask'\n",
        "                    else:\n",
        "                        # Find the first dataset\n",
        "                        for key in seg_f.keys():\n",
        "                            if isinstance(seg_f[key], h5py.Dataset):\n",
        "                                seg_key = key\n",
        "                                break\n",
        "                        else:\n",
        "                            raise ValueError(\"Could not find a dataset in the segmentation file\")\n",
        "\n",
        "                    # Load entire segmentation if it's not too large\n",
        "                    seg_size = os.path.getsize(segmentation_path) / (1024 * 1024 * 1024)\n",
        "                    if seg_size < 1.0:  # Less than 1GB\n",
        "                        segmentation = seg_f[seg_key][:]\n",
        "                    else:\n",
        "                        # Process segmentation in chunks\n",
        "                        print(\"Large segmentation detected, finding synapse coordinates first...\")\n",
        "                        # Find connected components in chunks\n",
        "                        synapse_coords = []\n",
        "\n",
        "                        # Process in slabs along z-axis\n",
        "                        chunk_size = min(50, volume_shape[0] // 10)  # Process ~10 chunks or fewer\n",
        "                        for z_start in range(0, volume_shape[0], chunk_size):\n",
        "                            z_end = min(z_start + chunk_size, volume_shape[0])\n",
        "                            print(f\"Processing segmentation chunk {z_start}-{z_end}...\")\n",
        "\n",
        "                            # Load chunk of segmentation\n",
        "                            seg_chunk = seg_f[seg_key][z_start:z_end, :, :]\n",
        "\n",
        "                            # Find synapses in this chunk\n",
        "                            labeled_chunk, num_features = ndi.label(seg_chunk)\n",
        "\n",
        "                            # For each synapse in this chunk\n",
        "                            for i in range(1, num_features + 1):\n",
        "                                # Get coordinates\n",
        "                                coords = np.where(labeled_chunk == i)\n",
        "                                if len(coords[0]) == 0:\n",
        "                                    continue\n",
        "\n",
        "                                # Adjust z coordinates for chunk offset\n",
        "                                mean_coords = [\n",
        "                                    coords[0].mean() + z_start,\n",
        "                                    coords[1].mean(),\n",
        "                                    coords[2].mean()\n",
        "                                ]\n",
        "\n",
        "                                synapse_coords.append(mean_coords)\n",
        "\n",
        "                        # Create placeholder for synapse regions\n",
        "                        patches = []\n",
        "\n",
        "                        # Extract each synapse from the volume\n",
        "                        patch_size = 64  # Fixed size for each synapse patch\n",
        "                        for coords in tqdm(synapse_coords, desc=\"Extracting synapse patches\"):\n",
        "                            z, y, x = [int(c) for c in coords]\n",
        "\n",
        "                            # Create bounds with padding\n",
        "                            z_min = max(0, z - patch_size // 2)\n",
        "                            z_max = min(volume_shape[0], z + patch_size // 2)\n",
        "                            y_min = max(0, y - patch_size // 2)\n",
        "                            y_max = min(volume_shape[1], y + patch_size // 2)\n",
        "                            x_min = max(0, x - patch_size // 2)\n",
        "                            x_max = min(volume_shape[2], x + patch_size // 2)\n",
        "\n",
        "                            # Extract patch from volume\n",
        "                            with h5py.File(volume_path, 'r') as vol_f:\n",
        "                                patch = vol_f[volume_key][z_min:z_max, y_min:y_max, x_min:x_max]\n",
        "\n",
        "                                # Take middle slice or max projection for 2D representation\n",
        "                                if patch.shape[0] > 0:\n",
        "                                    middle_slice = patch[patch.shape[0] // 2]\n",
        "                                    patches.append(middle_slice)\n",
        "\n",
        "                        # Skip the regular extraction process\n",
        "                        print(f\"Extracted {len(patches)} synapse patches directly\")\n",
        "                        # Predict types directly\n",
        "                        synapse_types = []\n",
        "                        confidences = []\n",
        "\n",
        "                        for patch in tqdm(patches, desc=\"Predicting synapse types\"):\n",
        "                            syn_type, conf = self.predict_synapse_type(patch, threshold=confidence_threshold)\n",
        "                            synapse_types.append(syn_type)\n",
        "                            confidences.append(conf)\n",
        "\n",
        "                        # Skip to visualization\n",
        "                        goto_visualization = True\n",
        "        else:\n",
        "            # Standard loading for smaller volumes\n",
        "            with h5py.File(volume_path, 'r') as f:\n",
        "                # Find the volume dataset\n",
        "                if 'em_volume' in f:\n",
        "                    volume = f['em_volume'][:]\n",
        "                else:\n",
        "                    # Find the first dataset\n",
        "                    for key in f.keys():\n",
        "                        if isinstance(f[key], h5py.Dataset):\n",
        "                            volume = f[key][:]\n",
        "                            break\n",
        "                    else:\n",
        "                        raise ValueError(\"Could not find a dataset in the volume file\")\n",
        "\n",
        "            print(f\"Loading segmentation from {segmentation_path}...\")\n",
        "            with h5py.File(segmentation_path, 'r') as f:\n",
        "                if 'synapse_mask' in f:\n",
        "                    segmentation = f['synapse_mask'][:]\n",
        "                else:\n",
        "                    # Find the first dataset\n",
        "                    for key in f.keys():\n",
        "                        if isinstance(f[key], h5py.Dataset):\n",
        "                            segmentation = f[key][:]\n",
        "                            break\n",
        "                    else:\n",
        "                        raise ValueError(\"Could not find a dataset in the segmentation file\")\n",
        "\n",
        "            # Standard extraction process\n",
        "            patches = self.extract_synapse_regions(volume, segmentation)\n",
        "            print(f\"Extracted {len(patches)} synapse patches\")\n",
        "\n",
        "            # Find connected components in the segmentation\n",
        "            labeled_mask, num_features = ndi.label(segmentation)\n",
        "\n",
        "            # Predict synapse types\n",
        "            synapse_types = []\n",
        "            confidences = []\n",
        "            synapse_coords = []\n",
        "\n",
        "            for i in tqdm(range(1, num_features + 1), desc=\"Predicting synapse types\"):\n",
        "                # Get coordinates for this synapse\n",
        "                coords = np.mean(np.where(labeled_mask == i), axis=1)\n",
        "                synapse_coords.append(coords)\n",
        "\n",
        "                # Get the patch for this synapse\n",
        "                patch_idx = i - 1  # 0-based indexing for patches\n",
        "                if patch_idx < len(patches):\n",
        "                    patch = patches[patch_idx]\n",
        "\n",
        "                    # Predict the type\n",
        "                    syn_type, conf = self.predict_synapse_type(patch, threshold=confidence_threshold)\n",
        "                    synapse_types.append(syn_type)\n",
        "                    confidences.append(conf)\n",
        "                else:\n",
        "                    # Skip this synapse if patch not available\n",
        "                    synapse_types.append(\"unknown\")\n",
        "                    confidences.append(0.0)\n",
        "\n",
        "            goto_visualization = False\n",
        "\n",
        "        # Visualization and analysis\n",
        "        if 'goto_visualization' not in locals() or not goto_visualization:\n",
        "            # Filter by confidence if not already done\n",
        "            confident_indices = [i for i, conf in enumerate(confidences) if conf >= confidence_threshold]\n",
        "            if confident_indices:  # Check if any confident predictions exist\n",
        "                filtered_types = [synapse_types[i] for i in confident_indices]\n",
        "                filtered_coords = [synapse_coords[i] for i in confident_indices]\n",
        "                filtered_confidences = [confidences[i] for i in confident_indices]\n",
        "                print(f\"Using {len(filtered_types)} synapses with confidence >= {confidence_threshold}\")\n",
        "            else:\n",
        "                print(\"Warning: No synapses met the confidence threshold. Using all synapses.\")\n",
        "                filtered_types = synapse_types\n",
        "                filtered_coords = synapse_coords\n",
        "                filtered_confidences = confidences\n",
        "        else:\n",
        "            # These were already filtered by confidence threshold in the prediction step\n",
        "            filtered_types = synapse_types\n",
        "            filtered_coords = synapse_coords\n",
        "            filtered_confidences = confidences\n",
        "\n",
        "        # Check if we have any synapses to visualize\n",
        "        if not filtered_types:\n",
        "            print(\"No synapses found to visualize. Returning empty results.\")\n",
        "            return {\n",
        "                'synapse_counts': {},\n",
        "                'synapse_coords': [],\n",
        "                'synapse_types': [],\n",
        "                'synapse_confidences': [],\n",
        "                'e_i_ratio': 0.0,\n",
        "                'total_confident_synapses': 0,\n",
        "                'visualization_files': []\n",
        "            }\n",
        "\n",
        "        # Count the number of each type\n",
        "        synapse_counts = pd.Series(filtered_types).value_counts()\n",
        "        print(\"\\nSynapse Type Distribution:\")\n",
        "        for syn_type, count in synapse_counts.items():\n",
        "            percentage = (count / len(filtered_types)) * 100\n",
        "            print(f\"  {syn_type.capitalize()}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "        # Create enhanced visualizations\n",
        "\n",
        "        # 1. 3D interactive visualization with plotly if available\n",
        "        try:\n",
        "            import plotly.graph_objects as go\n",
        "            from plotly.subplots import make_subplots\n",
        "\n",
        "            # Create interactive 3D plot\n",
        "            fig = make_subplots(\n",
        "                rows=1, cols=2,\n",
        "                specs=[[{'type': 'scatter3d'}, {'type': 'pie'}]],\n",
        "                subplot_titles=('Synapse Distribution in 3D', 'Synapse Type Proportions')\n",
        "            )\n",
        "\n",
        "            # Define colors for each type\n",
        "            color_map = {\n",
        "                'excitatory': 'red',\n",
        "                'inhibitory': 'blue',\n",
        "                'uncertain': 'gray',\n",
        "                'unknown': 'black'\n",
        "            }\n",
        "\n",
        "            # Create separate traces for each type for better interactive legend\n",
        "            for syn_type in set(filtered_types):\n",
        "                # Get indices for this type\n",
        "                type_indices = [i for i, t in enumerate(filtered_types) if t == syn_type]\n",
        "\n",
        "                # Extract coordinates and confidences for this type\n",
        "                type_coords = [filtered_coords[i] for i in type_indices]\n",
        "                type_confs = [filtered_confidences[i] for i in type_indices]\n",
        "\n",
        "                if not type_coords:\n",
        "                    continue\n",
        "\n",
        "                # Create 3D scatter plot for this type\n",
        "                x = [coord[2] for coord in type_coords]\n",
        "                y = [coord[1] for coord in type_coords]\n",
        "                z = [coord[0] for coord in type_coords]\n",
        "\n",
        "                # Size points by confidence\n",
        "                marker_size = [conf * 10 + 5 for conf in type_confs]\n",
        "\n",
        "                fig.add_trace(\n",
        "                    go.Scatter3d(\n",
        "                        x=x, y=y, z=z,\n",
        "                        mode='markers',\n",
        "                        marker=dict(\n",
        "                            size=marker_size,\n",
        "                            color=color_map.get(syn_type, 'purple'),\n",
        "                            opacity=0.8,\n",
        "                            symbol='circle'\n",
        "                        ),\n",
        "                        name=f\"{syn_type.capitalize()} Synapses\",\n",
        "                        hovertext=[f\"Type: {syn_type}<br>Confidence: {conf:.2f}\" for conf in type_confs]\n",
        "                    ),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "            # Add pie chart of proportions\n",
        "            labels = synapse_counts.index.tolist()\n",
        "            values = synapse_counts.values.tolist()\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Pie(\n",
        "                    labels=labels,\n",
        "                    values=values,\n",
        "                    textinfo='label+percent',\n",
        "                    marker=dict(\n",
        "                        colors=[color_map.get(label, 'purple') for label in labels]\n",
        "                    )\n",
        "                ),\n",
        "                row=1, col=2\n",
        "            )\n",
        "\n",
        "            # Update layout\n",
        "            fig.update_layout(\n",
        "                title_text=\"Synapse Distribution Analysis\",\n",
        "                height=800,\n",
        "                width=1200,\n",
        "                scene=dict(\n",
        "                    xaxis_title='X',\n",
        "                    yaxis_title='Y',\n",
        "                    zaxis_title='Z'\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Save as HTML for interactive exploration\n",
        "            html_output = os.path.splitext(output_file)[0] + '.html'\n",
        "            fig.write_html(html_output)\n",
        "            print(f\"Interactive 3D visualization saved to {html_output}\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Plotly not available, falling back to Matplotlib for visualization\")\n",
        "        except Exception as viz_error:\n",
        "            print(f\"Error creating interactive 3D visualization: {viz_error}\")\n",
        "            print(\"Falling back to standard Matplotlib visualization\")\n",
        "\n",
        "        # 2. Create a 3D visualization with matplotlib\n",
        "        try:\n",
        "            fig = plt.figure(figsize=(12, 10))\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "            # Plot each synapse with varying size based on confidence\n",
        "            for i, (coords, syn_type, conf) in enumerate(zip(filtered_coords, filtered_types, filtered_confidences)):\n",
        "                # Determine color based on synapse type\n",
        "                if syn_type == 'excitatory':\n",
        "                    color = 'red'\n",
        "                elif syn_type == 'inhibitory':\n",
        "                    color = 'blue'\n",
        "                elif syn_type == 'uncertain':\n",
        "                    color = 'gray'\n",
        "                else:\n",
        "                    color = 'black'\n",
        "\n",
        "                # Size based on confidence\n",
        "                size = conf * 50 + 20\n",
        "\n",
        "                # Plot with alpha for better visualization of overlapping points\n",
        "                ax.scatter(\n",
        "                    coords[2], coords[1], coords[0],\n",
        "                    c=color,\n",
        "                    s=size,\n",
        "                    alpha=0.7,\n",
        "                    edgecolors='white',\n",
        "                    linewidth=0.5\n",
        "                )\n",
        "\n",
        "            ax.set_xlabel('X')\n",
        "            ax.set_ylabel('Y')\n",
        "            ax.set_zlabel('Z')\n",
        "\n",
        "            # Better title with statistics\n",
        "            ax.set_title(f'Distribution of Synapses\\n'\n",
        "                        f'Excitatory: {synapse_counts.get(\"excitatory\", 0)} | '\n",
        "                        f'Inhibitory: {synapse_counts.get(\"inhibitory\", 0)} | '\n",
        "                        f'Uncertain: {synapse_counts.get(\"uncertain\", 0)}')\n",
        "\n",
        "            # Add a legend\n",
        "            from matplotlib.lines import Line2D\n",
        "\n",
        "            legend_elements = []\n",
        "            for syn_type, color in [\n",
        "                ('Excitatory', 'red'),\n",
        "                ('Inhibitory', 'blue'),\n",
        "                ('Uncertain', 'gray'),\n",
        "                ('Unknown', 'black')\n",
        "            ]:\n",
        "                if syn_type.lower() in synapse_counts:\n",
        "                    legend_elements.append(\n",
        "                        Line2D([0], [0], marker='o', color='w',\n",
        "                               label=f'{syn_type} ({synapse_counts.get(syn_type.lower(), 0)})',\n",
        "                               markerfacecolor=color, markersize=10)\n",
        "                    )\n",
        "\n",
        "            if legend_elements:  # Only add legend if we have elements\n",
        "                ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1))\n",
        "\n",
        "            # Set axis limits with padding\n",
        "            if filtered_coords:\n",
        "                x_coords = [coord[2] for coord in filtered_coords]\n",
        "                y_coords = [coord[1] for coord in filtered_coords]\n",
        "                z_coords = [coord[0] for coord in filtered_coords]\n",
        "\n",
        "                if x_coords and y_coords and z_coords:  # Check if coordinates are not empty\n",
        "                    padding = 20  # Add padding around min/max\n",
        "                    ax.set_xlim(min(x_coords) - padding, max(x_coords) + padding)\n",
        "                    ax.set_ylim(min(y_coords) - padding, max(y_coords) + padding)\n",
        "                    ax.set_zlim(min(z_coords) - padding, max(z_coords) + padding)\n",
        "\n",
        "            # Improve grid and ticks for better visibility\n",
        "            ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(output_file, dpi=300)\n",
        "            plt.close()  # Close figure to free memory\n",
        "            print(f\"3D visualization saved to {output_file}\")\n",
        "\n",
        "            # 3. Create a 2D density map for each axis plane\n",
        "            plt.figure(figsize=(18, 6))\n",
        "\n",
        "            # XY Plane (view from top)\n",
        "            plt.subplot(1, 3, 1)\n",
        "            for syn_type in ['excitatory', 'inhibitory']:\n",
        "                type_indices = [i for i, t in enumerate(filtered_types) if t == syn_type]\n",
        "                if not type_indices:\n",
        "                    continue\n",
        "\n",
        "                type_coords = [filtered_coords[i] for i in type_indices]\n",
        "                xs = [coord[2] for coord in type_coords]\n",
        "                ys = [coord[1] for coord in type_coords]\n",
        "\n",
        "                color = 'red' if syn_type == 'excitatory' else 'blue'\n",
        "                plt.scatter(xs, ys, c=color, alpha=0.5, label=syn_type.capitalize())\n",
        "\n",
        "            plt.xlabel('X')\n",
        "            plt.ylabel('Y')\n",
        "            plt.title('Synapse Distribution (XY Plane)')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # XZ Plane\n",
        "            plt.subplot(1, 3, 2)\n",
        "            for syn_type in ['excitatory', 'inhibitory']:\n",
        "                type_indices = [i for i, t in enumerate(filtered_types) if t == syn_type]\n",
        "                if not type_indices:\n",
        "                    continue\n",
        "\n",
        "                type_coords = [filtered_coords[i] for i in type_indices]\n",
        "                xs = [coord[2] for coord in type_coords]\n",
        "                zs = [coord[0] for coord in type_coords]\n",
        "\n",
        "                color = 'red' if syn_type == 'excitatory' else 'blue'\n",
        "                plt.scatter(xs, zs, c=color, alpha=0.5, label=syn_type.capitalize())\n",
        "\n",
        "            plt.xlabel('X')\n",
        "            plt.ylabel('Z')\n",
        "            plt.title('Synapse Distribution (XZ Plane)')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # YZ Plane\n",
        "            plt.subplot(1, 3, 3)\n",
        "            for syn_type in ['excitatory', 'inhibitory']:\n",
        "                type_indices = [i for i, t in enumerate(filtered_types) if t == syn_type]\n",
        "                if not type_indices:\n",
        "                    continue\n",
        "\n",
        "                type_coords = [filtered_coords[i] for i in type_indices]\n",
        "                ys = [coord[1] for coord in type_coords]\n",
        "                zs = [coord[0] for coord in type_coords]\n",
        "\n",
        "                color = 'red' if syn_type == 'excitatory' else 'blue'\n",
        "                plt.scatter(ys, zs, c=color, alpha=0.5, label=syn_type.capitalize())\n",
        "\n",
        "            plt.xlabel('Y')\n",
        "            plt.ylabel('Z')\n",
        "            plt.title('Synapse Distribution (YZ Plane)')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            planes_output = os.path.splitext(output_file)[0] + '_planes.png'\n",
        "            plt.savefig(planes_output, dpi=300)\n",
        "            plt.close()  # Close figure to free memory\n",
        "            print(f\"2D plane visualizations saved to {planes_output}\")\n",
        "\n",
        "        except Exception as matplotlib_error:\n",
        "            print(f\"Error creating matplotlib visualizations: {matplotlib_error}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            print(\"Visualization failed, but continuing with data analysis\")\n",
        "            planes_output = None  # No planes output if visualization failed\n",
        "\n",
        "        # Create distribution statistics dictionary\n",
        "        e_i_ratio = synapse_counts.get('excitatory', 0) / max(1, synapse_counts.get('inhibitory', 0))\n",
        "        viz_files = [output_file]\n",
        "        if planes_output:\n",
        "            viz_files.append(planes_output)\n",
        "\n",
        "        distribution = {\n",
        "            'synapse_counts': synapse_counts.to_dict(),\n",
        "            'synapse_coords': filtered_coords,\n",
        "            'synapse_types': filtered_types,\n",
        "            'synapse_confidences': filtered_confidences,\n",
        "            'e_i_ratio': e_i_ratio,\n",
        "            'total_confident_synapses': len(filtered_types),\n",
        "            'visualization_files': viz_files\n",
        "        }\n",
        "\n",
        "        # Save the distribution data as JSON for future reference\n",
        "        try:\n",
        "            import json\n",
        "\n",
        "            # Convert numpy arrays to lists for JSON serialization\n",
        "            json_safe_distribution = {\n",
        "                'synapse_counts': distribution['synapse_counts'],\n",
        "                'e_i_ratio': float(e_i_ratio),\n",
        "                'total_confident_synapses': distribution['total_confident_synapses'],\n",
        "                'visualization_files': distribution['visualization_files'],\n",
        "                # Don't save coordinates/types in JSON (too large)\n",
        "            }\n",
        "\n",
        "            json_output = os.path.splitext(output_file)[0] + '_stats.json'\n",
        "            with open(json_output, 'w') as f:\n",
        "                json.dump(json_safe_distribution, f, indent=2)\n",
        "\n",
        "            print(f\"Distribution statistics saved to {json_output}\")\n",
        "        except Exception as json_error:\n",
        "            print(f\"Error saving JSON statistics: {json_error}\")\n",
        "\n",
        "        return distribution\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error mapping synapse distribution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Return basic error info\n",
        "        return {\n",
        "            'error': str(e),\n",
        "            'synapse_counts': {},\n",
        "            'total_synapses': 0\n",
        "        }"
      ],
      "metadata": {
        "id": "jxE6MZYZ4QpC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gB-3LBUe5CHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_results[f'precision_{label}'].append(class_metrics['precision'])\n",
        "                            cv_results[f'recall_{label}'].append(class_metrics['recall'])\n",
        "                            cv_results[f'f1_{label}'].append(class_metrics['f1-score'])\n",
        "\n",
        "                    # Free memory\n",
        "                    del self.model\n",
        "                    gc.collect()\n",
        "\n",
        "                # Calculate means and standard deviations\n",
        "                cv_summary = {}\n",
        "                for metric, values in cv_results.items():\n",
        "                    if metric != 'confusion_matrices':\n",
        "                        cv_summary[f'{metric}_mean'] = np.mean(values)\n",
        "                        cv_summary[f'{metric}_std'] = np.std(values)\n",
        "\n",
        "                # Save cross-validation results\n",
        "                with open(os.path.join(self.output_path, 'cv_results.pkl'), 'wb') as f:\n",
        "                    pickle.dump(cv_results, f)\n",
        "\n",
        "                # Create a summary report\n",
        "                cv_report = pd.DataFrame({\n",
        "                    'Metric': list(cv_summary.keys()),\n",
        "                    'Value': list(cv_summary.values())\n",
        "                })\n",
        "                cv_report.to_csv(os.path.join(self.output_path, 'cv_summary.csv'), index=False)\n",
        "\n",
        "                print(f\"\\nCross-validation completed in {time.time() - cv_start_time:.2f} seconds\")\n",
        "                print(\"\\nCross-validation Summary:\")\n",
        "                print(f\"  Mean Accuracy: {cv_summary['accuracy_mean']:.4f} ± {cv_summary['accuracy_std']:.4f}\")\n",
        "\n",
        "                # Plot cross-validation results\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                metrics_to_plot = ['accuracy']\n",
        "                for label in set([0, 1]):  # Binary classification\n",
        "                    metrics_to_plot.extend([f'precision_{label}', f'recall_{label}', f'f1_{label}'])\n",
        "\n",
        "                for i, metric in enumerate(metrics_to_plot):\n",
        "                    if metric in cv_results:\n",
        "                        plt.subplot(1, len(metrics_to_plot), i+1)\n",
        "                        plt.boxplot(cv_results[metric])\n",
        "                        plt.title(metric.replace('_', ' ').title())\n",
        "                        plt.ylim(0, 1)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(self.output_path, 'cv_results.png'))\n",
        "\n",
        "            # 3. Train on full dataset for final model\n",
        "            print(\"\\n3. Training final model on full dataset...\")\n",
        "            train_start_time = time.time()\n",
        "\n",
        "            # Split data for final evaluation\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42, stratify=y\n",
        "            )\n",
        "\n",
        "            # Build and train the final model\n",
        "            self.model = self.build_classification_model(use_pretrained=True)\n",
        "            history = self.train_model(X_train, y_train, epochs=50, fine_tune=True)\n",
        "\n",
        "            print(f\"Model training completed in {time.time() - train_start_time:.2f} seconds\")\n",
        "\n",
        "            # 4. Evaluate the final model\n",
        "            print(\"\\n4. Evaluating final model...\")\n",
        "            eval_start_time = time.time()\n",
        "            metrics = self.evaluate_model(X_test, y_test)\n",
        "            print(f\"Evaluation completed in {time.time() - eval_start_time:.2f} seconds\")\n",
        "\n",
        "            # 5. Visualize model attention on sample images\n",
        "            print(\"\\n5. Generating attention visualizations...\")\n",
        "            viz_start_time = time.time()\n",
        "\n",
        "            # Create a directory for visualizations\n",
        "            viz_dir = os.path.join(self.output_path, 'visualizations')\n",
        "            os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "            # Randomly select some test samples from each class\n",
        "            sample_indices = []\n",
        "            for class_label in np.unique(y_test):\n",
        "                class_indices = np.where(y_test == class_label)[0]\n",
        "                selected = np.random.choice(class_indices,\n",
        "                                           size=min(3, len(class_indices)),\n",
        "                                           replace=False)\n",
        "                sample_indices.extend(selected)\n",
        "\n",
        "            # Generate attention maps\n",
        "            for i, idx in enumerate(sample_indices):\n",
        "                try:\n",
        "                    viz_path = os.path.join(viz_dir, f'attention_sample_{i+1}.png')\n",
        "                    self.visualize_model_attention(\n",
        "                        X_test[idx],\n",
        "                        true_label=y_test[idx],\n",
        "                        save_path=viz_path\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating visualization for sample {i+1}: {e}\")\n",
        "\n",
        "            print(f\"Visualizations generated in {time.time() - viz_start_time:.2f} seconds\")\n",
        "\n",
        "            # 6. Save and summarize results\n",
        "            print(\"\\n6. Saving results and generating summary...\")\n",
        "\n",
        "            # Create learning curve plot\n",
        "            plt.figure(figsize=(12, 5))\n",
        "\n",
        "            # Plot training & validation accuracy\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(history['accuracy'], label='Train')\n",
        "            plt.plot(history['val_accuracy'], label='Validation')\n",
        "            plt.title('Model Accuracy')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot training & validation loss\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(history['loss'], label='Train')\n",
        "            plt.plot(history['val_loss'], label='Validation')\n",
        "            plt.title('Model Loss')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.legend(loc='upper right')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_path, 'learning_curves.png'), dpi=300)\n",
        "\n",
        "            # Save training history\n",
        "            with open(os.path.join(self.output_path, 'training_history.json'), 'w') as f:\n",
        "                # Convert numpy values to Python native types for JSON serialization\n",
        "                json_history = {}\n",
        "                for k, v in history.items():\n",
        "                    json_history[k] = [float(val) for val in v]\n",
        "                json.dump(json_history, f, indent=2)\n",
        "\n",
        "            # Save the results\n",
        "            results = {\n",
        "                'history': history,\n",
        "                'metrics': metrics,\n",
        "                'model_path': os.path.join(self.output_path, 'final_model'),\n",
        "                'training_time': time.time() - train_start_time,\n",
        "                'total_time': time.time() - pipeline_start_time\n",
        "            }\n",
        "\n",
        "            # Create a summary report\n",
        "            with open(os.path.join(self.output_path, 'results_summary.txt'), 'w') as f:\n",
        "                f.write(f\"Synapse Classification Results Summary\\n\")\n",
        "                f.write(f\"{'='*50}\\n\\n\")\n",
        "                f.write(f\"Dataset Information:\\n\")\n",
        "                f.write(f\"  Total samples: {len(X)}\\n\")\n",
        "                f.write(f\"  Training samples: {len(X_train)}\\n\")\n",
        "                f.write(f\"  Test samples: {len(X_test)}\\n\")\n",
        "                f.write(f\"  Class distribution: {pd.Series(y).value_counts().to_dict()}\\n\\n\")\n",
        "\n",
        "                f.write(f\"Model Performance:\\n\")\n",
        "                f.write(f\"  Test Accuracy: {metrics['accuracy']:.4f}\\n\\n\")\n",
        "\n",
        "                f.write(f\"Classification Report:\\n\")\n",
        "                class_report = classification_report(y_test, np.argmax(metrics['pred_probabilities'], axis=1))\n",
        "                f.write(f\"{class_report}\\n\\n\")\n",
        "\n",
        "                f.write(f\"Training Information:\\n\")\n",
        "                f.write(f\"  Training time: {results['training_time']:.2f} seconds\\n\")\n",
        "                f.write(f\"  Total pipeline time: {results['total_time']:.2f} seconds\\n\")\n",
        "                f.write(f\"  Final validation accuracy: {history['val_accuracy'][-1]:.4f}\\n\")\n",
        "                f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\\n\")\n",
        "\n",
        "                f.write(f\"Model files saved to: {self.output_path}\\n\")\n",
        "\n",
        "            print(f\"\\nPipeline completed successfully in {time.time() - pipeline_start_time:.2f} seconds!\")\n",
        "            print(f\"Results saved to {self.output_path}\")\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in pipeline execution: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'traceback': traceback.format_exc()\n",
        "            }"
      ],
      "metadata": {
        "id": "WbFE86JnTzzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _plot_training_history(self, history):\n",
        "        \"\"\"\n",
        "        Plot the training history with enhanced visualizations\n",
        "\n",
        "        Args:\n",
        "            history (dict): Training history\n",
        "        \"\"\"\n",
        "        # Create a multi-faceted visualization of training history\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        # 1. Accuracy plot\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(history['accuracy'], linewidth=2)\n",
        "        plt.plot(history['val_accuracy'], linewidth=2)\n",
        "        plt.title('Model Accuracy', fontsize=14)\n",
        "        plt.ylabel('Accuracy', fontsize=12)\n",
        "        plt.xlabel('Epoch', fontsize=12)\n",
        "        plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.ylim([0, 1.0])\n",
        "\n",
        "        # Highlight best validation accuracy\n",
        "        best_val_acc_epoch = np.argmax(history['val_accuracy'])\n",
        "        best_val_acc = history['val_accuracy'][best_val_acc_epoch]\n",
        "        plt.axhline(y=best_val_acc, color='r', linestyle='--', alpha=0.3)\n",
        "        plt.axvline(x=best_val_acc_epoch, color='r', linestyle='--', alpha=0.3)\n",
        "        plt.scatter(best_val_acc_epoch, best_val_acc, s=100, c='red', alpha=0.5, zorder=5)\n",
        "        plt.annotate(f'Best: {best_val_acc:.4f}',\n",
        "                    (best_val_acc_epoch, best_val_acc),\n",
        "                    xytext=(best_val_acc_epoch+1, best_val_acc),\n",
        "                    fontsize=10)\n",
        "\n",
        "        # 2. Loss plot\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(history['loss'], linewidth=2)\n",
        "        plt.plot(history['val_loss'], linewidth=2)\n",
        "        plt.title('Model Loss', fontsize=14)\n",
        "        plt.ylabel('Loss', fontsize=12)\n",
        "        plt.xlabel('Epoch', fontsize=12)\n",
        "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "        # Highlight best validation loss\n",
        "        best_val_loss_epoch = np.argmin(history['val_loss'])\n",
        "        best_val_loss = history['val_loss'][best_val_loss_epoch]\n",
        "        plt.axhline(y=best_val_loss, color='r', linestyle='--', alpha=0.3)\n",
        "        plt.axvline(x=best_val_loss_epoch, color='r', linestyle='--', alpha=0.3)\n",
        "        plt.scatter(best_val_loss_epoch, best_val_loss, s=100, c='red', alpha=0.5, zorder=5)\n",
        "        plt.annotate(f'Best: {best_val_loss:.4f}',\n",
        "                    (best_val_loss_epoch, best_val_loss),\n",
        "                    xytext=(best_val_loss_epoch+1, best_val_loss),\n",
        "                    fontsize=10)\n",
        "\n",
        "        # 3. Accuracy vs Loss scatter plot\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.scatter(history['loss'], history['accuracy'], alpha=0.5, s=70, c='blue', label='Train')\n",
        "        plt.scatter(history['val_loss'], history['val_accuracy'], alpha=0.5, s=70, c='red', label='Validation')\n",
        "\n",
        "        # Add arrows to show training progression\n",
        "        for i in range(len(history['loss'])-1):\n",
        "            plt.arrow(history['loss'][i], history['accuracy'][i],\n",
        "                    history['loss'][i+1] - history['loss'][i],\n",
        "                    history['accuracy'][i+1] - history['accuracy'][i],\n",
        "                    head_width=0.01, head_length=0.01, fc='blue', ec='blue', alpha=0.3)\n",
        "\n",
        "            plt.arrow(history['val_loss'][i], history['val_accuracy'][i],\n",
        "                    history['val_loss'][i+1] - history['val_loss'][i],\n",
        "                    history['val_accuracy'][i+1] - history['val_accuracy'][i],\n",
        "                    head_width=0.01, head_length=0.01, fc='red', ec='red', alpha=0.3)\n",
        "\n",
        "        plt.title('Accuracy vs Loss', fontsize=14)\n",
        "        plt.xlabel('Loss', fontsize=12)\n",
        "        plt.ylabel('Accuracy', fontsize=12)\n",
        "        plt.legend(loc='lower left')\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "        # 4. Learning rate (if available) or train vs val metrics\n",
        "        plt.subplot(2, 2, 4)\n",
        "        if 'lr' in history:\n",
        "            plt.semilogy(history['lr'], linewidth=2)\n",
        "            plt.title('Learning Rate', fontsize=14)\n",
        "            plt.ylabel('Learning Rate', fontsize=12)\n",
        "            plt.xlabel('Epoch', fontsize=12)\n",
        "            plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        else:\n",
        "            # Plot the difference between train and validation\n",
        "            train_val_acc_diff = np.array(history['accuracy']) - np.array(history['val_accuracy'])\n",
        "            train_val_loss_diff = np.array(history['loss']) - np.array(history['val_loss'])\n",
        "\n",
        "            plt.plot(train_val_acc_diff, label='Acc Diff (Train-Val)', color='green')\n",
        "            plt.plot(train_val_loss_diff, label='Loss Diff (Train-Val)', color='purple')\n",
        "            plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
        "            plt.title('Model Overfitting Indicators', fontsize=14)\n",
        "            plt.ylabel('Difference (Train-Val)', fontsize=12)\n",
        "            plt.xlabel('Epoch', fontsize=12)\n",
        "            plt.legend(loc='upper right')\n",
        "            plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_path, 'training_history.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the synapse classification pipeline with improved error handling\n",
        "    \"\"\"\n",
        "    import time\n",
        "    import argparse\n",
        "\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(description='Synapse Classification from EM Images')\n",
        "    parser.add_argument('--data_path', type=str, required=True, help='Path to the Microns dataset')\n",
        "    parser.add_argument('--output_path', type=str, default='./synapse_classification_results',\n",
        "                        help='Path to save results')\n",
        "    parser.add_argument('--sample_limit', type=int, default=None,\n",
        "                        help='Limit the number of samples (for testing)')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
        "    parser.add_argument('--image_size', type=int, default=224, help='Image size for processing')\n",
        "    parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs')\n",
        "    parser.add_argument('--cross_validation', action='store_true',\n",
        "                        help='Use cross-validation for evaluation')\n",
        "    parser.add_argument('--no_pretrained', action='store_true',\n",
        "                        help='Do not use pretrained model')\n",
        "    parser.add_argument('--test_only', action='store_true',\n",
        "                        help='Only run evaluation on existing model')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(args.output_path, exist_ok=True)\n",
        "\n",
        "        # Set up logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(os.path.join(args.output_path, 'pipeline.log')),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(f\"Starting synapse classification pipeline with args: {args}\")\n",
        "\n",
        "        # Initialize the classifier\n",
        "        start_time = time.time()\n",
        "        logger.info(f\"Initializing SynapseClassifier...\")\n",
        "\n",
        "        classifier = SynapseClassifier(\n",
        "            args.data_path,\n",
        "            args.output_path,\n",
        "            image_size=(args.image_size, args.image_size),\n",
        "            batch_size=args.batch_size\n",
        "        )\n",
        "\n",
        "        # Run the pipeline or load existing model for testing\n",
        "        if args.test_only:\n",
        "            logger.info(\"Test only mode activated. Loading existing model...\")\n",
        "            try:\n",
        "                # Try to load model in TensorFlow format first\n",
        "                model_path = os.path.join(args.output_path, 'final_model')\n",
        "                if os.path.exists(model_path):\n",
        "                    classifier.model = tf.keras.models.load_model(model_path)\n",
        "                else:\n",
        "                    # Try H5 format as fallback\n",
        "                    h5_path = os.path.join(args.output_path, 'final_model.h5')\n",
        "                    if os.path.exists(h5_path):\n",
        "                        classifier.model = tf.keras.models.load_model(h5_path)\n",
        "                    else:\n",
        "                        raise FileNotFoundError(\"No model file found in the output directory\")\n",
        "\n",
        "                logger.info(\"Model loaded successfully. Preparing test data...\")\n",
        "\n",
        "                # Load test data\n",
        "                X, y = classifier.load_microns_data(sample_limit=args.sample_limit)\n",
        "                X_train, X_test, y_train, y_test = train_test_split(\n",
        "                    X, y, test_size=0.2, random_state=42, stratify=y\n",
        "                )\n",
        "\n",
        "                # Evaluate the model\n",
        "                logger.info(\"Evaluating the model...\")\n",
        "                metrics = classifier.evaluate_model(X_test, y_test)\n",
        "\n",
        "                # Save evaluation results\n",
        "                with open(os.path.join(args.output_path, 'test_results.pkl'), 'wb') as f:\n",
        "                    pickle.dump(metrics, f)\n",
        "\n",
        "                logger.info(f\"Test completed in {time.time() - start_time:.2f} seconds\")\n",
        "                logger.info(f\"Test accuracy: {metrics['accuracy']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in test mode: {e}\")\n",
        "                import traceback\n",
        "                logger.error(traceback.format_exc())\n",
        "                sys.exit(1)\n",
        "\n",
        "        else:\n",
        "            # Run the full pipeline\n",
        "            logger.info(\"Running full pipeline...\")\n",
        "            results = classifier.run_full_pipeline(\n",
        "                sample_limit=args.sample_limit,\n",
        "                cross_validation=args.cross_validation\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Pipeline completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "            if 'error' in results:\n",
        "                logger.error(f\"Pipeline failed with error: {results['error']}\")\n",
        "                sys.exit(1)\n",
        "            else:\n",
        "                logger.info(f\"Pipeline completed successfully!\")\n",
        "                logger.info(f\"Test accuracy: {results['metrics']['accuracy']:.4f}\")\n",
        "\n",
        "        logger.info(f\"Results saved to {args.output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Add missing imports\n",
        "    import sys\n",
        "    import time\n",
        "    import logging\n",
        "    import json\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "KslswxLET1Uj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}